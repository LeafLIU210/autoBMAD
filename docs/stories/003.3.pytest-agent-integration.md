# Story 003.3: Pytest Agent Integration

**Epic Reference**: [Epic-003: Quality Gates Integration](../epics/epic-003-quality-gates-sprint.md)

---

## Status
**Status**: Approved
**Priority**: High
**Estimated Effort**: 3 days
**Assigned To**: Dev Agent

---

## Story

**As a** BMAD automation system,
**I want to** integrate pytest test automation with debugpy,
**so that** test execution can be automated with debugging for persistent failures.

---

## Acceptance Criteria

- [ ] Pytest and debugpy added as pip dependencies
- [ ] TestAutomationAgent class created
- [ ] Pytest execution: `pytest -v --tb=short --json-report {test_dir}`
- [ ] JSON report parsing for test failures
- [ ] Debugpy invocation for persistent failures
- [ ] Claude SDK fixes for test issues (using max_turns=150, no external timeouts)
- [ ] Max 3 cycles with 2 retries each (no asyncio.wait_for or asyncio.shield)
- [ ] Zero Cancel Scope errors in implementation

---

## Tasks / Subtasks

- [ ] Task 1: Add pytest and debugpy dependencies
  - [ ] Subtask 1.1: Update pyproject.toml with pytest and debugpy
  - [ ] Subtask 1.2: Install dependencies in virtual environment

- [ ] Task 2: Create TestAutomationAgent class
  - [ ] Subtask 2.1: Design TestAutomationAgent architecture
  - [ ] Subtask 2.2: Implement pytest execution method
  - [ ] Subtask 2.3: Implement JSON report parsing

- [ ] Task 3: Implement pytest test execution workflow
  - [ ] Subtask 3.1: Execute `pytest -v --tb=short --json-report {test_dir}`
  - [ ] Subtask 3.2: Parse JSON report for test failures
  - [ ] Subtask 3.3: Extract failure details and error messages

- [ ] Task 4: Implement debugpy integration
  - [ ] Subtask 4.1: Add debugpy invocation for persistent failures
  - [ ] Subtask 4.2: Create debugging workflow for failing tests
  - [ ] Subtask 4.3: Integrate debugpy with test execution

- [ ] Task 5: Implement test fix workflow
  - [ ] Subtask 5.1: Use Claude SDK to generate test fixes
  - [ ] Subtask 5.2: Apply fixes to test files
  - [ ] Subtask 5.3: Re-run tests after fixes

- [ ] Task 6: Test and validate
  - [ ] Subtask 6.1: Create unit tests for TestAutomationAgent
  - [ ] Subtask 6.2: Test pytest integration with sample tests
  - [ ] Subtask 6.3: Test debugpy invocation workflow

---

## Dev Notes

### Epic-003 Integration Context
This story is the third and final agent in the quality gates pipeline. It executes after both Ruff and Basedpyright agents complete. The complete pipeline: Ruff → Basedpyright → **Pytest**.

### Technical Architecture
- **Class Location**: Create in `autoBMAD/epic_automation/test_automation_agent.py`
- **Activation**: Triggered after basedpyright agent completion
- **JSON Report**: Essential for programmatic test failure parsing
- **Debugpy Integration**: For persistent test failures (5+ failed fix attempts)

### TestAutomationAgent Design Pattern
```python
class TestAutomationAgent:
    def __init__(self):
        self.test_executor = PytestExecutor()
        self.debugger = DebugpyIntegration()
        self.fix_generator = ClaudeSDKFixer()

    def execute_tests(self, test_dir):
        # Execute pytest with JSON report
        # Parse test results
        # Return structured data

    def handle_failures(self, failures):
        # Determine if debugpy needed (5+ failures)
        # Use Claude SDK to generate fixes
        # Apply fixes and re-run tests

    def invoke_debugpy(self, persistent_failures):
        # Launch debugpy for failing tests
        # Interactive debugging session
        # Return debug results
```

### Relevant Source Tree
- **Main Integration**: `autoBMAD/epic_automation/epic_driver.py` (calls this agent last)
- **Agent Location**: `autoBMAD/epic_automation/test_automation_agent.py`
- **Test Directory**: `tests/` directory for test execution

### Key Technical Details
1. **Test Execution**: Use `pytest -v --tb=short --json-report {test_dir}` for JSON output
2. **Failure Parsing**: Extract test failure information from JSON report
3. **Debugpy Invocation**: Trigger after 5+ failed fix attempts
4. **SDK Integration**: Claude SDK with `max_turns=150` for test fixes (NO external timeouts)
5. **Cycle Management**: 3-cycle retry system, independent of quality agents

### ⚠️ Cancel Scope Safety Requirements
- **DO NOT use** `asyncio.wait_for()` or `asyncio.shield()` for SDK calls
- **DO use** `max_turns=150` in ClaudeAgentOptions for protection
- **NO external timeout mechanisms** - let SDK sessions complete naturally
- **Simple exception handling** - catch exceptions without external cancellation
- **Reference**: See `docs/evaluation/cancel-scope-error-analysis.md` for details

### Debugpy Integration Details
- **Trigger Condition**: 5 consecutive failed fix attempts on same test
- **Debug Session**: Interactive debugging to identify root causes
- **Integration**: Connect debugpy with pytest execution
- **Logging**: Capture debug session output for analysis

### Sequential Workflow Position
- **Prerequisites**: Ruff and Basedpyright must complete successfully
- **Code State**: Works on code after quality fixes (ruff + basedpyright)
- **Test State**: Executes existing test suite, not creates new tests
- **Success Criteria**: All tests pass after automated fixes

---

## Testing

### Testing Standards
- **Test File Location**: `tests/epic_automation/test_test_automation_agent.py`
- **Test Framework**: pytest
- **Test Coverage**: ≥90% for TestAutomationAgent class

### Test Requirements
1. **Unit Tests**:
   - Test pytest command execution
   - Test JSON report parsing
   - Test debugpy invocation logic
   - Test fix generation workflow

2. **Integration Tests**:
   - Test complete pytest execution cycle
   - Test debugpy integration with failing tests
   - Test fix application and re-execution
   - Test sequential execution after quality agents

3. **Test Data**:
   - Create sample test files with intentional failures
   - Include various test types (unit, integration)
   - Include both passing and failing scenarios
   - Verify debugpy activation conditions

### Special Testing Considerations
- **Debugpy Testing**: Requires careful setup for CI environments
- **Test Isolation**: Ensure test execution doesn't affect other tests
- **Timeout Handling**: Manage long-running debug sessions
- **Error Recovery**: Test behavior when debugpy fails

---

## Dev Agent Record

### Agent Model Used
_This section will be populated by the development agent during implementation_

### Debug Log References
_This section will be populated by the development agent during implementation_

### Completion Notes List
_This section will be populated by the development agent during implementation_

### File List
_This section will be populated by the development agent during implementation_

---

## QA Results

_This section will be populated by the QA agent during review_

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-07 | 1.1 | Added Cancel Scope safety requirements - removed external timeouts, added SDK max_turns protection | Product Owner |
| 2026-01-07 | 1.0 | Initial story creation for Pytest Agent Integration | Scrum Master (Bob) |

---
