### Status
**Done**

# Story 1.3: Comprehensive Testing Suite

**Status:** Ready for Development

## Epic Reference
- **Epic:** Core Algorithm Foundation
- **Epic ID:** 1
- **Story ID:** 1.3

## User Story

**As a developer,**
I want thorough unit tests for the bubble sort implementation,
so that I can be confident the algorithm works correctly for all edge cases.

## Acceptance Criteria

1. **Test Structure**
   - [ ] Create test directory at `tests/test_core/`
   - [ ] Create test file `tests/test_core/test_sorting.py`
   - [ ] Create `tests/__init__.py` and `tests/test_core/__init__.py`
   - [ ] Use pytest framework for all tests

2. **Basic Functionality Tests**
   - [ ] Test empty list handling
   - [ ] Test single element list
   - [ ] Test two element list
   - [ ] Test multiple element list (7+ elements)
   - [ ] All tests verify correct sorting

3. **Edge Case Tests**
   - [ ] Test already sorted list (best case)
   - [ ] Test reverse sorted list (worst case)
   - [ ] Test list with all duplicate values
   - [ ] Test list with some duplicate values
   - [ ] Test list with negative numbers
   - [ ] Test list with floating point numbers
   - [ ] Test list with mixed positive and negative numbers
   - [ ] Test list with very large numbers (1000+ elements)

4. **Error Handling Tests**
   - [ ] Test that None input raises TypeError
   - [ ] Test that non-iterable input raises TypeError
   - [ ] Test that non-comparable mixed types raise appropriate error
   - [ ] Verify all error messages are descriptive and helpful

5. **Function Purity Tests**
   - [ ] Test that original input list is never modified
   - [ ] Verify function returns a new list instance
   - [ ] Test with mutable list to ensure no side effects

6. **Test Coverage**
   - [ ] Achieve >95% code coverage for the sorting module
   - [ ] All code paths tested (including error handling)
   - [ ] All branches tested (if/else, try/except blocks)
   - [ ] All exception paths tested

7. **Parameterized Tests**
   - [ ] Use `@pytest.mark.parametrize` for multiple input scenarios
   - [ ] Parameterize basic functionality tests
   - [ ] Parameterize edge case tests
   - [ ] Reduce test code duplication

8. **Test Organization**
   - [ ] Use descriptive test method names
   - [ ] Group related tests using pytest marks
   - [ ] Include docstrings for complex test scenarios
   - [ ] Organize tests logically (basic → edge cases → errors)

## Tasks/Subtasks

### Task 1.3.1: Setup Test Framework
- **Estimate:** 0.5 days
- **Priority:** High
- **Dependencies:** Story 1.2
- **Details:**
  - Verify pytest is installed
  - Configure pytest in `pyproject.toml`
  - Setup pytest-cov for coverage reporting
  - Create test directory structure

### Task 1.3.2: Create Basic Functionality Tests
- **Estimate:** 1 day
- **Priority:** High
- **Dependencies:** Task 1.3.1
- **Details:**
  - Write tests for empty list
  - Write tests for single element
  - Write tests for two elements
  - Write tests for multiple elements
  - Use parameterized tests where appropriate

### Task 1.3.3: Create Edge Case Tests
- **Estimate:** 1 day
- **Priority:** High
- **Dependencies:** Task 1.3.2
- **Details:**
  - Write tests for already sorted lists
  - Write tests for reverse sorted lists
  - Write tests for duplicate values
  - Write tests for negative numbers
  - Write tests for floating point numbers
  - Write tests for large datasets

### Task 1.3.4: Create Error Handling Tests
- **Estimate:** 0.5 days
- **Priority:** High
- **Dependencies:** Task 1.3.3
- **Details:**
  - Write tests for None input
  - Write tests for non-iterable input
  - Write tests for mixed type input
  - Verify error messages are correct

### Task 1.3.5: Create Function Purity Tests
- **Estimate:** 0.5 days
- **Priority:** Medium
- **Dependencies:** Task 1.3.4
- **Details:**
  - Write tests verifying input preservation
  - Write tests verifying new list is returned
  - Test with mutable objects

### Task 1.3.6: Optimize and Parameterize Tests
- **Estimate:** 0.5 days
- **Priority:** Medium
- **Dependencies:** Task 1.3.5
- **Details:**
  - Refactor tests to use parametrize decorator
  - Reduce code duplication
  - Improve test organization
  - Add helpful comments

## Dev Notes

### Test File Structure
```python
# tests/test_core/test_sorting.py
import pytest
from bubble_sort.core.sorting import bubble_sort

class TestBubbleSort:
    """Test suite for bubble_sort function."""

    # Basic functionality tests
    def test_empty_list(self):
        """Test that empty list returns empty list."""
        result = bubble_sort([])
        assert result == []

    # Use parametrize for multiple test cases
    @pytest.mark.parametrize("input_list, expected", [
        ([1], [1]),
        ([2, 1], [1, 2]),
        ([3, 1, 2], [1, 2, 3]),
        ([5, 2, 8, 1, 9], [1, 2, 5, 8, 9]),
    ])
    def test_basic_sorting(self, input_list, expected):
        """Test basic sorting functionality."""
        result = bubble_sort(input_list)
        assert result == expected

    # Edge cases
    @pytest.mark.parametrize("input_list", [
        [1, 2, 3, 4, 5],  # Already sorted
        [5, 4, 3, 2, 1],  # Reverse sorted
        [1, 1, 1, 1],     # All duplicates
        [5, 2, 5, 2, 5], # Some duplicates
    ])
    def test_edge_cases(self, input_list):
        """Test various edge cases."""
        result = bubble_sort(input_list.copy())
        assert result == sorted(input_list)
```

### Pytest Configuration
Add to `pyproject.toml`:
```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--verbose",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
]

[tool.coverage.run]
source = ["src"]
omit = ["*/tests/*", "*/test_*"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]
```

### Testing Best Practices
1. **Test Naming:** Use descriptive test names that explain what's being tested
2. **Arrange-Act-Assert:** Structure tests clearly
3. **One Assertion Per Test:** Keep tests focused (except for parametrize cases)
4. **Test Independence:** Each test should be able to run independently
5. **Use Fixtures:** Create reusable test data with pytest fixtures
6. **Clear Assertions:** Use specific assertion messages when helpful

### Coverage Requirements
- Target: >95% coverage
- Measure: Use `pytest-cov`
- Report: Generate HTML coverage report
- Exclude: Only exclude code that truly cannot be tested (e.g., `if False:`)

### Error Testing Pattern
```python
def test_none_input_raises_error(self):
    """Test that None input raises TypeError."""
    with pytest.raises(TypeError, match="Input cannot be None"):
        bubble_sort(None)

def test_non_iterable_raises_error(self):
    """Test that non-iterable input raises TypeError."""
    with pytest.raises(TypeError, match="Input must be an iterable"):
        bubble_sort(42)
```

## Testing

### Test Categories

#### 1. Basic Functionality Tests
**Purpose:** Verify core sorting behavior
- Empty list returns empty list
- Single element returns same element
- Multiple elements sorted correctly
- Two elements swapped if out of order

#### 2. Edge Case Tests
**Purpose:** Verify correctness in special scenarios
- Already sorted (best case for optimization)
- Reverse sorted (worst case)
- All duplicates
- Some duplicates
- Negative numbers
- Floating point numbers
- Large datasets

#### 3. Error Handling Tests
**Purpose:** Verify graceful error handling
- None input
- Non-iterable input
- Mixed types (if not supported)

#### 4. Purity Tests
**Purpose:** Verify no side effects
- Input list unchanged
- New list instance returned

#### 5. Performance Tests (Optional)
**Purpose:** Verify basic performance characteristics
- Large list (1000+ elements) completes in reasonable time
- Early termination optimization works (sorted list processes faster than reverse sorted)

### Test Execution Commands
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=bubble_sort --cov-report=html --cov-report=term

# Run specific test file
pytest tests/test_core/test_sorting.py

# Run with verbose output
pytest -v

# Run only fast tests
pytest -m "not slow"

# Generate coverage report
pytest --cov=bubble_sort --cov-report=html
# Open htmlcov/index.html in browser
```

### Expected Test Count
- Basic functionality: 4-5 tests
- Edge cases: 8-10 tests (parameterized)
- Error handling: 3-4 tests
- Purity: 2-3 tests
- **Total:** ~20-25 test cases

### Coverage Report Goals
- Line coverage: >95%
- Branch coverage: >90%
- Function coverage: 100%
- All exception paths tested

## Definition of Done

- [ ] All acceptance criteria met
- [ ] All tasks completed and reviewed
- [ ] Test suite passes with 100% success rate
- [ ] Code coverage >95% achieved
- [ ] All edge cases covered by tests
- [ ] Error handling fully tested
- [ ] Function purity verified
- [ ] Tests are well-organized and readable
- [ ] Parameterized tests reduce code duplication
- [ ] pytest configuration complete
- [ ] Coverage report generated and reviewed
- [ ] CI/CD pipeline runs tests successfully
- [ ] Tests can be run locally with simple commands
- [ ] Documentation updated with testing instructions

## Notes

- Tests should be comprehensive but not overly verbose
- Use fixtures for common test data
- Consider using hypothesis for property-based testing (optional enhancement)
- Keep tests fast to encourage frequent execution
- Document any assumptions or special test cases
- Ensure tests work across all supported Python versions (3.8+)
