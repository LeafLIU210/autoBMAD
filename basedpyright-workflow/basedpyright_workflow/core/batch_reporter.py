"""æ‰¹é‡æŠ¥å‘Šç”Ÿæˆå™¨æ¨¡å—

æ‰©å±•ç°æœ‰çš„æŠ¥å‘Šç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†æŠ¥å‘Šã€è¶‹åŠ¿åˆ†æå’Œå¤šæ–‡ä»¶æ¯”è¾ƒã€‚
åŸºäºåŸæœ‰çš„ ReportGeneratorï¼Œå¢åŠ æ‰¹é‡å¤„ç†èƒ½åŠ›ã€‚
"""

import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional
from collections import defaultdict, Counter
from dataclasses import dataclass, asdict

from .batch_processor import BatchErrorProcessor


@dataclass
class BatchReportConfig:
    """æ‰¹é‡æŠ¥å‘Šé…ç½®"""
    include_trends: bool = True
    include_file_comparison: bool = True
    include_category_analysis: bool = True
    include_fix_recommendations: bool = True
    max_error_details: int = 50
    max_files_in_summary: int = 20


@dataclass
class TrendAnalysis:
    """è¶‹åŠ¿åˆ†ææ•°æ®"""
    period: str
    error_count: int
    warning_count: int
    file_count: int
    unique_errors: int
    auto_fixable: int
    timestamp: datetime


@dataclass
class FileComparison:
    """æ–‡ä»¶æ¯”è¾ƒæ•°æ®"""
    file_path: str
    current_errors: int
    previous_errors: int
    change: int
    change_percentage: float
    trend: str  # "improving", "worsening", "stable"


class BatchReportGenerator:
    """æ‰¹é‡æŠ¥å‘Šç”Ÿæˆå™¨

    æ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š
    1. å¤šä¸ªé”™è¯¯æ–‡ä»¶çš„æ±‡æ€»æŠ¥å‘Š
    2. å†å²æ•°æ®è¶‹åŠ¿åˆ†æ
    3. æ–‡ä»¶çº§åˆ«çš„å˜åŒ–è¿½è¸ª
    4. å¢å¼ºçš„é”™è¯¯åˆ†æå’Œå»ºè®®
    5. å¯é…ç½®çš„æŠ¥å‘Šå†…å®¹
    """

    def __init__(self, config: Optional[BatchReportConfig] = None):
        """åˆå§‹åŒ–æ‰¹é‡æŠ¥å‘Šç”Ÿæˆå™¨

        Args:
            config: æ‰¹é‡æŠ¥å‘Šé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or BatchReportConfig()
        self.processors: List[BatchErrorProcessor] = []
        self.reports: List[Dict] = []
        self.trend_data: List[TrendAnalysis] = []

    def add_processor(self, processor: BatchErrorProcessor) -> None:
        """æ·»åŠ æ‰¹é‡å¤„ç†å™¨åˆ°æŠ¥å‘Šç”Ÿæˆå™¨

        Args:
            processor: å·²å®Œæˆåˆ†æçš„æ‰¹é‡å¤„ç†å™¨
        """
        if processor.stats:
            self.processors.append(processor)

    def load_from_files(self, error_files: List[Path]) -> bool:
        """ä»å¤šä¸ªé”™è¯¯æ–‡ä»¶åŠ è½½æ•°æ®

        Args:
            error_files: é”™è¯¯æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½æ‰€æœ‰æ–‡ä»¶
        """
        success = True
        for file_path in error_files:
            try:
                processor = BatchErrorProcessor(file_path)
                if processor.load_errors():
                    processor.analyze()
                    self.add_processor(processor)
                    print(f"å·²åŠ è½½: {file_path.name}")
                else:
                    print(f"åŠ è½½å¤±è´¥: {file_path}")
                    success = False
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                success = False

        return success

    def load_historical_data(self, results_dir: Path, days: int = 30) -> None:
        """åŠ è½½å†å²æ•°æ®ç”¨äºè¶‹åŠ¿åˆ†æ

        Args:
            results_dir: ç»“æœç›®å½•è·¯å¾„
            days: è¦åˆ†æçš„å¤©æ•°
        """
        cutoff_date = datetime.now() - timedelta(days=days)

        # æŸ¥æ‰¾å†å²æŠ¥å‘Šæ–‡ä»¶
        pattern = "batch_analysis_report_*.json"
        historical_files = []

        for file_path in results_dir.glob(pattern):
            try:
                # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³
                timestamp_str = file_path.stem.split('_')[-1]
                file_date = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")

                if file_date >= cutoff_date:
                    historical_files.append((file_path, file_date))
            except ValueError:
                continue

        # æŒ‰æ—¶é—´æ’åº
        historical_files.sort(key=lambda x: x[1])

        # ç”Ÿæˆè¶‹åŠ¿æ•°æ®
        for file_path, file_date in historical_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                stats = data.get('statistics', {})
                trend = TrendAnalysis(
                    period=file_date.strftime("%Y-%m-%d"),
                    error_count=stats.get('total_errors', 0),
                    warning_count=stats.get('total_warnings', 0),
                    file_count=len(stats.get('by_file', {})),
                    unique_errors=stats.get('unique_errors', 0),
                    auto_fixable=stats.get('auto_fixable_count', 0),
                    timestamp=file_date
                )
                self.trend_data.append(trend)

            except Exception as e:
                print(f"å¤„ç†å†å²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def _calculate_file_comparisons(self) -> List[FileComparison]:
        """è®¡ç®—æ–‡ä»¶çº§åˆ«çš„æ¯”è¾ƒæ•°æ®

        Returns:
            List[FileComparison]: æ–‡ä»¶æ¯”è¾ƒæ•°æ®åˆ—è¡¨
        """
        if not self.trend_data or len(self.processors) == 0:
            return []

        # è·å–å½“å‰å’Œä¹‹å‰çš„é”™è¯¯æ•°æ®
        current_processor = self.processors[-1]  # æœ€æ–°çš„å¤„ç†å™¨
        current_file_errors = Counter(error.file for error in current_processor.processed_errors)

        # å¦‚æœæœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨æœ€è¿‘çš„å†å²ç‚¹ä½œä¸ºæ¯”è¾ƒåŸºå‡†
        if self.trend_data:
            # æŸ¥æ‰¾æœ€è¿‘çš„åŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯çš„å†å²æŠ¥å‘Š
            previous_file_errors = Counter()
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä»å†å²æŠ¥å‘Šä¸­è§£æè¯¦ç»†é”™è¯¯ä¿¡æ¯

        comparisons = []
        for file_path, current_count in current_file_errors.items():
            previous_count = previous_file_errors.get(file_path, 0)
            change = current_count - previous_count
            change_percentage = (change / previous_count * 100) if previous_count > 0 else 100

            if change > 0:
                trend = "worsening"
            elif change < 0:
                trend = "improving"
            else:
                trend = "stable"

            comparison = FileComparison(
                file_path=file_path,
                current_errors=current_count,
                previous_errors=previous_count,
                change=change,
                change_percentage=change_percentage,
                trend=trend
            )
            comparisons.append(comparison)

        return sorted(comparisons, key=lambda x: abs(x.change), reverse=True)

    def _generate_trend_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¶‹åŠ¿åˆ†ææ‘˜è¦

        Returns:
            Dict: è¶‹åŠ¿åˆ†ææ‘˜è¦
        """
        if not self.trend_data:
            return {}

        if len(self.trend_data) < 2:
            return {
                "period_count": len(self.trend_data),
                "latest_period": self.trend_data[0].period,
                "message": "éœ€è¦æ›´å¤šæ•°æ®ç‚¹æ¥è¿›è¡Œè¶‹åŠ¿åˆ†æ"
            }

        latest = self.trend_data[-1]
        earliest = self.trend_data[0]

        error_trend = "stable"
        if latest.error_count > earliest.error_count * 1.1:
            error_trend = "increasing"
        elif latest.error_count < earliest.error_count * 0.9:
            error_trend = "decreasing"

        return {
            "period_count": len(self.trend_data),
            "date_range": f"{earliest.period} to {latest.period}",
            "error_trend": error_trend,
            "error_change": latest.error_count - earliest.error_count,
            "error_change_percentage": (
                (latest.error_count - earliest.error_count) / earliest.error_count * 100
                if earliest.error_count > 0 else 0
            ),
            "auto_fixable_trend": "improving" if latest.auto_fixable > earliest.auto_fixable else "stable"
        }

    def generate_comprehensive_markdown(self, output_file: Path) -> Path:
        """ç”Ÿæˆç»¼åˆæ‰¹é‡å¤„ç† Markdown æŠ¥å‘Š

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            Path: ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        lines = []
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æŠ¥å‘Šå¤´éƒ¨
        lines.append("# BasedPyright æ‰¹é‡å¤„ç†ç»¼åˆæŠ¥å‘Š\n\n")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {timestamp}\n")
        lines.append(f"**åˆ†ææ–‡ä»¶æ•°é‡**: {len(self.processors)}\n")
        lines.append(f"**æŠ¥å‘Šé…ç½®**: è¶‹åŠ¿åˆ†æ={'å¯ç”¨' if self.config.include_trends else 'ç¦ç”¨'}, "
                   f"æ–‡ä»¶æ¯”è¾ƒ={'å¯ç”¨' if self.config.include_file_comparison else 'ç¦ç”¨'}\n\n")

        # æ‰§è¡Œæ‘˜è¦
        self._add_executive_summary(lines)

        # è¶‹åŠ¿åˆ†æ
        if self.config.include_trends and self.trend_data:
            self._add_trend_analysis_section(lines)

        # æ–‡ä»¶æ¯”è¾ƒåˆ†æ
        if self.config.include_file_comparison:
            self._add_file_comparison_section(lines)

        # é”™è¯¯åˆ†ç±»åˆ†æ
        if self.config.include_category_analysis:
            self._add_category_analysis_section(lines)

        # ä¿®å¤å»ºè®®å’Œä¼˜å…ˆçº§
        if self.config.include_fix_recommendations:
            self._add_fix_recommendations_section(lines)

        # è¯¦ç»†é”™è¯¯åˆ—è¡¨
        self._add_detailed_errors_section(lines)

        # é™„å½•å’Œç»Ÿè®¡
        self._add_appendix_section(lines)

        # å†™å…¥æ–‡ä»¶
        output_file.parent.mkdir(parents=True, exist_ok=True)
        output_file.write_text("".join(lines), encoding="utf-8")

        return output_file

    def _add_executive_summary(self, lines: List[str]) -> None:
        """æ·»åŠ æ‰§è¡Œæ‘˜è¦éƒ¨åˆ†"""
        lines.append("## ğŸ“Š æ‰§è¡Œæ‘˜è¦\n\n")

        if not self.processors:
            lines.append("æœªæ‰¾åˆ°å¯åˆ†æçš„æ•°æ®ã€‚\n\n")
            return

        # æ±‡æ€»ç»Ÿè®¡
        total_errors = sum(p.stats.total_errors for p in self.processors if p.stats)
        total_unique = sum(p.stats.unique_errors for p in self.processors if p.stats)
        total_auto_fixable = sum(p.stats.auto_fixable_count for p in self.processors if p.stats)
        total_files = len(set(
            error.file
            for processor in self.processors
            for error in processor.processed_errors
        ))

        lines.append("| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |\n")
        lines.append("|------|------|------|\n")
        lines.append(f"| ğŸ“ æ¶‰åŠæ–‡ä»¶ | {total_files} | å­˜åœ¨é”™è¯¯çš„æºä»£ç æ–‡ä»¶æ•°é‡ |\n")
        lines.append(f"| âŒ æ€»é”™è¯¯æ•° | {total_errors} | æ‰€æœ‰æ£€æŸ¥ä¸­çš„åŸå§‹é”™è¯¯æ€»æ•° |\n")
        lines.append(f"| ğŸ”„ å»é‡é”™è¯¯ | {total_unique} | å»é‡åçš„å”¯ä¸€é”™è¯¯æ•°é‡ |\n")
        lines.append(f"| ğŸ”§ å¯è‡ªåŠ¨ä¿®å¤ | {total_auto_fixable} | ç½®ä¿¡åº¦è¾ƒé«˜çš„å¯ä¿®å¤é”™è¯¯ |\n")
        lines.append(f"| ğŸ“ˆ ä¿®å¤ç‡ | {total_auto_fixable/total_unique*100:.1f}% | å¯ä¿®å¤é”™è¯¯å æ¯” |\n\n")

        # æ•´ä½“è¯„ä¼°
        if total_unique == 0:
            status = "âœ… ä¼˜ç§€"
            recommendation = "ä»£ç è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ"
        elif total_auto_fixable / total_unique > 0.7:
            status = "âš ï¸ éœ€è¦å…³æ³¨"
            recommendation = "å¤§éƒ¨åˆ†é”™è¯¯å¯ä»¥è‡ªåŠ¨ä¿®å¤ï¼Œå»ºè®®è¿è¡Œæ‰¹é‡ä¿®å¤"
        elif total_auto_fixable / total_unique > 0.3:
            status = "ğŸ”´ éœ€è¦å¤„ç†"
            recommendation = "éœ€è¦äººå·¥å®¡æŸ¥å’Œè‡ªåŠ¨ä¿®å¤ç›¸ç»“åˆ"
        else:
            status = "ğŸš¨ é«˜ä¼˜å…ˆçº§"
            recommendation = "é”™è¯¯è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦ä»”ç»†åˆ†æå¹¶åˆ¶å®šä¿®å¤è®¡åˆ’"

        lines.append(f"### æ•´ä½“è¯„ä¼°: {status}\n\n")
        lines.append(f"**å»ºè®®**: {recommendation}\n\n")

    def _add_trend_analysis_section(self, lines: List[str]) -> None:
        """æ·»åŠ è¶‹åŠ¿åˆ†æéƒ¨åˆ†"""
        lines.append("## ğŸ“ˆ è¶‹åŠ¿åˆ†æ\n\n")

        trend_summary = self._generate_trend_summary()
        if not trend_summary:
            lines.append("æš‚æ— è¶³å¤Ÿçš„å†å²æ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æã€‚\n\n")
            return

        lines.append(f"**åˆ†æå‘¨æœŸ**: {trend_summary.get('date_range', 'N/A')}\n")
        lines.append(f"**æ•°æ®ç‚¹æ•°é‡**: {trend_summary.get('period_count', 0)}\n\n")

        # è¶‹åŠ¿æŒ‡æ ‡
        lines.append("### å…³é”®è¶‹åŠ¿\n\n")
        lines.append("| æŒ‡æ ‡ | è¶‹åŠ¿ | å˜åŒ– |\n")
        lines.append("|------|------|------|\n")

        error_trend = trend_summary.get('error_trend', 'stable')
        trend_icons = {"increasing": "ğŸ“ˆ", "decreasing": "ğŸ“‰", "stable": "â¡ï¸"}
        error_change = trend_summary.get('error_change', 0)

        lines.append(f"| é”™è¯¯æ•°é‡ | {trend_icons.get(error_trend, 'â¡ï¸')} {error_trend} | {error_change:+d} |\n")

        auto_fixable_trend = trend_summary.get('auto_fixable_trend', 'stable')
        lines.append(f"| å¯ä¿®å¤æ€§ | {trend_icons.get(auto_fixable_trend, 'â¡ï¸')} {auto_fixable_trend} | - |\n\n")

        # è¶‹åŠ¿å›¾è¡¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if len(self.trend_data) > 1:
            lines.append("### å†å²è¶‹åŠ¿å›¾\n\n")
            lines.append("```\n")
            lines.append("é”™è¯¯æ•°é‡è¶‹åŠ¿:")
            for i, trend in enumerate(self.trend_data[-10:]):  # æ˜¾ç¤ºæœ€è¿‘10ä¸ªæ•°æ®ç‚¹
                bar_length = min(50, trend.error_count)
                bar = "â–ˆ" * bar_length
                lines.append(f"{trend.period} | {bar} {trend.error_count}")
            lines.append("```\n\n")

    def _add_file_comparison_section(self, lines: List[str]) -> None:
        """æ·»åŠ æ–‡ä»¶æ¯”è¾ƒéƒ¨åˆ†"""
        lines.append("## ğŸ“ æ–‡ä»¶çº§åˆ«åˆ†æ\n\n")

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„é”™è¯¯ç»Ÿè®¡
        file_stats = defaultdict(lambda: {'errors': 0, 'categories': defaultdict(int)})

        for processor in self.processors:
            for error in processor.processed_errors:
                file_stats[error.file]['errors'] += 1
                file_stats[error.file]['categories'][error.category.value] += 1

        # æŒ‰é”™è¯¯æ•°é‡æ’åº
        sorted_files = sorted(file_stats.items(), key=lambda x: x[1]['errors'], reverse=True)

        lines.append(f"å…±æ¶‰åŠ **{len(sorted_files)}** ä¸ªæ–‡ä»¶\n\n")

        # Top æ–‡ä»¶è¡¨æ ¼
        lines.append("### é”™è¯¯æ•°é‡æœ€å¤šçš„æ–‡ä»¶\n\n")
        lines.append("| æ’å | æ–‡ä»¶è·¯å¾„ | é”™è¯¯æ•° | ç®€å•é”™è¯¯ | å¤æ‚é”™è¯¯ | éœ€äººå·¥å®¡æŸ¥ |\n")
        lines.append("|------|----------|--------|----------|----------|------------|\n")

        for i, (file_path, stats) in enumerate(sorted_files[:self.config.max_files_in_summary], 1):
            simple = stats['categories']['simple']
            complex_err = stats['categories']['complex']
            manual = stats['categories']['manual']

            # æˆªæ–­è¿‡é•¿çš„æ–‡ä»¶è·¯å¾„
            display_path = file_path if len(file_path) <= 50 else "..." + file_path[-47:]

            lines.append(f"| {i} | `{display_path}` | {stats['errors']} | {simple} | {complex_err} | {manual} |\n")

        lines.append("\n")

    def _add_category_analysis_section(self, lines: List[str]) -> None:
        """æ·»åŠ é”™è¯¯åˆ†ç±»åˆ†æéƒ¨åˆ†"""
        lines.append("## ğŸ·ï¸ é”™è¯¯åˆ†ç±»åˆ†æ\n\n")

        # æ±‡æ€»æ‰€æœ‰å¤„ç†å™¨çš„åˆ†ç±»ç»Ÿè®¡
        category_totals = defaultdict(int)
        severity_totals = defaultdict(int)

        for processor in self.processors:
            if processor.stats:
                for category, count in processor.stats.by_category.items():
                    category_totals[category] += count
                for severity, count in processor.stats.by_severity.items():
                    severity_totals[severity] += count

        total_errors = sum(category_totals.values())

        if total_errors == 0:
            lines.append("æ²¡æœ‰å‘ç°é”™è¯¯ã€‚\n\n")
            return

        # åˆ†ç±»ç»Ÿè®¡è¡¨æ ¼
        lines.append("### é”™è¯¯åˆ†ç±»åˆ†å¸ƒ\n\n")
        lines.append("| åˆ†ç±» | æ•°é‡ | å æ¯” | ä¿®å¤éš¾åº¦ |\n")
        lines.append("|------|------|------|----------|\n")

        difficulty_map = {
            "simple": "ğŸŸ¢ ç®€å•",
            "complex": "ğŸŸ¡ ä¸­ç­‰",
            "manual": "ğŸ”´ å¤æ‚"
        }

        for category in ["simple", "complex", "manual"]:
            count = category_totals.get(category, 0)
            percentage = count / total_errors * 100 if total_errors > 0 else 0
            difficulty = difficulty_map.get(category, "æœªçŸ¥")

            lines.append(f"| {difficulty} | {count} | {percentage:.1f}% | {category} |\n")

        lines.append("\n")

        # ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        lines.append("### ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ\n\n")
        lines.append("| ä¸¥é‡ç¨‹åº¦ | æ•°é‡ | å æ¯” | ä¼˜å…ˆçº§ |\n")
        lines.append("|----------|------|------|--------|\n")

        priority_map = {
            "critical": "ğŸš¨ ç«‹å³",
            "high": "ğŸ”´ é«˜",
            "medium": "ğŸŸ¡ ä¸­",
            "low": "ğŸŸ¢ ä½"
        }

        for severity in ["critical", "high", "medium", "low"]:
            count = severity_totals.get(severity, 0)
            percentage = count / total_errors * 100 if total_errors > 0 else 0
            priority = priority_map.get(severity, "æœªçŸ¥")

            lines.append(f"| {priority} {severity} | {count} | {percentage:.1f}% | ç«‹å³ä¿®å¤ if severity == 'critical' else 'é«˜ä¼˜å…ˆçº§' if severity == 'high' else 'ä¸­ç­‰ä¼˜å…ˆçº§' if severity == 'medium' else 'ä½ä¼˜å…ˆçº§' |\n")

        lines.append("\n")

    def _add_fix_recommendations_section(self, lines: List[str]) -> None:
        """æ·»åŠ ä¿®å¤å»ºè®®éƒ¨åˆ†"""
        lines.append("## ğŸ”§ ä¿®å¤å»ºè®®å’Œä¼˜å…ˆçº§\n\n")

        # æ”¶é›†æ‰€æœ‰å¯è‡ªåŠ¨ä¿®å¤çš„é”™è¯¯
        auto_fixable_errors = []
        for processor in self.processors:
            auto_fixable_errors.extend(processor.get_auto_fixable_errors())

        if not auto_fixable_errors:
            lines.append("å½“å‰æ²¡æœ‰å¯è‡ªåŠ¨ä¿®å¤çš„é”™è¯¯ã€‚\n\n")
            return

        lines.append(f"å‘ç° **{len(auto_fixable_errors)}** ä¸ªå¯è‡ªåŠ¨ä¿®å¤çš„é”™è¯¯\n\n")

        # æŒ‰ç½®ä¿¡åº¦åˆ†ç»„
        high_confidence = [e for e in auto_fixable_errors if e.confidence >= 0.9]
        medium_confidence = [e for e in auto_fixable_errors if 0.7 <= e.confidence < 0.9]
        low_confidence = [e for e in auto_fixable_errors if e.confidence < 0.7]

        lines.append("### æŒ‰ç½®ä¿¡åº¦åˆ†ç»„\n\n")
        lines.append(f"- ğŸŸ¢ é«˜ç½®ä¿¡åº¦ (â‰¥90%): {len(high_confidence)} ä¸ªé”™è¯¯")
        lines.append(f"- ğŸŸ¡ ä¸­ç½®ä¿¡åº¦ (70-89%): {len(medium_confidence)} ä¸ªé”™è¯¯")
        lines.append(f"- ğŸ”´ ä½ç½®ä¿¡åº¦ (<70%): {len(low_confidence)} ä¸ªé”™è¯¯\n\n")

        # ä¿®å¤æ“ä½œå»ºè®®
        lines.append("### æ¨èä¿®å¤æ“ä½œ\n\n")

        if high_confidence:
            lines.append("1. **ç«‹å³è‡ªåŠ¨ä¿®å¤** (é«˜ç½®ä¿¡åº¦é”™è¯¯)\n")
            lines.append("   ```bash\n")
            lines.append("   basedpyright batch-fix --auto\n")
            lines.append("   ```\n\n")

        if medium_confidence:
            lines.append("2. **äº¤äº’å¼ä¿®å¤** (ä¸­ç½®ä¿¡åº¦é”™è¯¯)\n")
            lines.append("   ```bash\n")
            lines.append("   basedpyright batch-fix\n")
            lines.append("   ```\n\n")

        if low_confidence:
            lines.append("3. **äººå·¥å®¡æŸ¥** (ä½ç½®ä¿¡åº¦é”™è¯¯)\n")
            lines.append("   å»ºè®®æ‰‹åŠ¨æ£€æŸ¥è¿™äº›é”™è¯¯ï¼Œç¡®è®¤ä¿®å¤å»ºè®®çš„å‡†ç¡®æ€§\n\n")

        # å¸¸è§é”™è¯¯æ¨¡å¼
        lines.append("### å¸¸è§é”™è¯¯æ¨¡å¼\n\n")
        error_patterns = defaultdict(int)
        for error in auto_fixable_errors:
            # æå–é”™è¯¯æ¨¡å¼ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            pattern = error.rule
            error_patterns[pattern] += 1

        for pattern, count in sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)[:5]:
            lines.append(f"- **{pattern}**: {count} ä¸ªé”™è¯¯\n")

        lines.append("\n")

    def _add_detailed_errors_section(self, lines: List[str]) -> None:
        """æ·»åŠ è¯¦ç»†é”™è¯¯åˆ—è¡¨éƒ¨åˆ†"""
        lines.append("## ğŸ“‹ è¯¦ç»†é”™è¯¯åˆ—è¡¨\n\n")

        # æ”¶é›†æ‰€æœ‰é”™è¯¯å¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
        all_errors = []
        for processor in self.processors:
            all_errors.extend(processor.processed_errors)

        if not all_errors:
            lines.append("æ²¡æœ‰å‘ç°é”™è¯¯ã€‚\n\n")
            return

        # æŒ‰ä¸¥é‡ç¨‹åº¦å’Œç±»åˆ«æ’åº
        severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        category_order = {"simple": 0, "complex": 1, "manual": 2}

        def sort_key(error):
            return (
                severity_order.get(error.severity_level.value, 3),
                category_order.get(error.category.value, 2),
                -error.confidence
            )

        sorted_errors = sorted(all_errors, key=sort_key)

        # æ˜¾ç¤ºå‰Nä¸ªæœ€é‡è¦çš„é”™è¯¯
        display_count = min(len(sorted_errors), self.config.max_error_details)
        lines.append(f"æ˜¾ç¤ºå‰ {display_count} ä¸ªé«˜ä¼˜å…ˆçº§é”™è¯¯ï¼ˆå…± {len(sorted_errors)} ä¸ªï¼‰\n\n")

        for i, error in enumerate(sorted_errors[:display_count], 1):
            severity_icon = {"critical": "ğŸš¨", "high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}
            category_icon = {"simple": "ğŸŸ¢", "complex": "ğŸŸ¡", "manual": "ğŸ”´"}

            icon = f"{severity_icon.get(error.severity_level.value, 'â“')} {category_icon.get(error.category.value, 'â“')}"

            lines.append(f"### {i}. {icon} {error.file}:{error.line}\n\n")
            lines.append(f"**é”™è¯¯ä¿¡æ¯**: {error.message}\n\n")
            lines.append(f"**è§„åˆ™**: `{error.rule}`\n")
            lines.append(f"**ä¸¥é‡ç¨‹åº¦**: {error.severity_level.value}\n")
            lines.append(f"**åˆ†ç±»**: {error.category.value}\n")
            lines.append(f"**ç½®ä¿¡åº¦**: {error.confidence:.1%}\n")

            if error.fix_suggestion:
                lines.append(f"**ä¿®å¤å»ºè®®**: {error.fix_suggestion}\n")

            lines.append("\n---\n\n")

    def _add_appendix_section(self, lines: List[str]) -> None:
        """æ·»åŠ é™„å½•éƒ¨åˆ†"""
        lines.append("## ğŸ“– é™„å½•\n\n")

        # æŠ€æœ¯ä¿¡æ¯
        lines.append("### æŠ€æœ¯ä¿¡æ¯\n\n")
        lines.append("- **æŠ¥å‘Šç”Ÿæˆå™¨ç‰ˆæœ¬**: BatchReportGenerator v1.0\n")
        lines.append("- **åˆ†æå™¨ç‰ˆæœ¬**: BatchErrorProcessor\n")
        lines.append(f"- **ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}\n")
        lines.append(f"- **é…ç½®é€‰é¡¹**: {asdict(self.config)}\n\n")

        # ä½¿ç”¨è¯´æ˜
        lines.append("### ä½¿ç”¨è¯´æ˜\n\n")
        lines.append("1. **è‡ªåŠ¨ä¿®å¤**: è¿è¡Œ `basedpyright batch-fix --auto` ä¿®å¤é«˜ç½®ä¿¡åº¦é”™è¯¯\n")
        lines.append("2. **äº¤äº’ä¿®å¤**: è¿è¡Œ `basedpyright batch-fix` é€ä¸ªç¡®è®¤ä¿®å¤\n")
        lines.append("3. **é‡æ–°åˆ†æ**: è¿è¡Œ `basedpyright batch-analyze` é‡æ–°ç”Ÿæˆåˆ†ææŠ¥å‘Š\n")
        lines.append("4. **æŒç»­ç›‘æ§**: å®šæœŸè¿è¡Œæ£€æŸ¥ä»¥è·Ÿè¸ªä»£ç è´¨é‡è¶‹åŠ¿\n\n")

        # æ•…éšœæ’é™¤
        lines.append("### æ•…éšœæ’é™¤\n\n")
        lines.append("- å¦‚æœä¿®å¤å»ºè®®ä¸å‡†ç¡®ï¼Œè¯·æ£€æŸ¥å…·ä½“çš„é”™è¯¯ä¸Šä¸‹æ–‡\n")
        lines.append("- å¤æ‚é”™è¯¯å¯èƒ½éœ€è¦é‡æ–°è®¾è®¡ç±»å‹ç³»ç»Ÿæˆ–ä»£ç ç»“æ„\n")
        lines.append("- å»ºè®®åœ¨ä¿®å¤å‰åˆ›å»ºä»£ç åˆ†æ”¯ä»¥ä¾¿å›æ»š\n")
        lines.append("- å¯ä»¥é€šè¿‡æ›´æ–°åˆ†ç±»è§„åˆ™æ¥æ”¹è¿›é”™è¯¯åˆ†æå‡†ç¡®æ€§\n\n")

        lines.append("---\n")
        lines.append(f"*æŠ¥å‘Šç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")

    def generate_summary_report(self, output_file: Path) -> Path:
        """ç”Ÿæˆç®€åŒ–çš„æ‘˜è¦æŠ¥å‘Š

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            Path: ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        lines = []
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        lines.append("# BasedPyright æ‰¹é‡å¤„ç†æ‘˜è¦\n\n")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {timestamp}\n\n")

        if not self.processors:
            lines.append("æœªæ‰¾åˆ°å¯åˆ†æçš„æ•°æ®ã€‚\n\n")
            output_file.write_text("".join(lines), encoding="utf-8")
            return output_file

        # å¿«é€Ÿç»Ÿè®¡
        total_errors = sum(p.stats.total_errors for p in self.processors if p.stats)
        total_unique = sum(p.stats.unique_errors for p in self.processors if p.stats)
        total_auto_fixable = sum(p.stats.auto_fixable_count for p in self.processors if p.stats)

        lines.append("## ğŸ“Š å¿«é€Ÿç»Ÿè®¡\n\n")
        lines.append(f"- æ€»é”™è¯¯æ•°: {total_errors}\n")
        lines.append(f"- å»é‡é”™è¯¯: {total_unique}\n")
        lines.append(f"- å¯ä¿®å¤: {total_auto_fixable} ({total_auto_fixable/total_unique*100:.1f}%)\n\n")

        # ä¸‹ä¸€æ­¥æ“ä½œ
        lines.append("## ğŸ¯ å»ºè®®æ“ä½œ\n\n")

        if total_auto_fixable > 0:
            lines.append("1. **ç«‹å³ä¿®å¤å¯è‡ªåŠ¨ä¿®å¤çš„é”™è¯¯**:\n")
            lines.append("   ```bash\n")
            lines.append("   basedpyright batch-fix --auto\n")
            lines.append("   ```\n\n")

        lines.append("2. **æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š**:\n")
        lines.append("   è¿è¡Œ `basedpyright batch-report` ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š\n\n")

        lines.append("3. **æŒç»­ç›‘æ§**:\n")
        lines.append("   å®šæœŸè¿è¡Œæ£€æŸ¥ä»¥è·Ÿè¸ªä»£ç è´¨é‡å˜åŒ–\n\n")

        output_file.write_text("".join(lines), encoding="utf-8")
        return output_file