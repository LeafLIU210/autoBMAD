"""
è´¨é‡æ£€æŸ¥ä»£ç†ä¿®å¤æµ‹è¯•

æµ‹è¯•é’ˆå¯¹ä»¥ä¸‹ä¿®å¤çš„å•å…ƒæµ‹è¯•ï¼š
1. ä¿®å¤ RuffAgent/BasedPyrightAgent è¿”å›å€¼é”™è¯¯
2. ä¿®å¤ PytestAgent å¤±è´¥ä¿¡æ¯ç±»å‹è½¬æ¢
3. ä¿®å¤ Unicode è§£ç é”™è¯¯

ä½œè€…: autoBMAD Team
æ—¥æœŸ: 2026-01-14
"""

import pytest
import json
import logging
import tempfile
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch, MagicMock

from autoBMAD.epic_automation.agents.quality_agents import (
    RuffAgent,
    BasedPyrightAgent,
    PytestAgent,
    PytestTestCase
)


class TestQualityAgentFixes:
    """è´¨é‡æ£€æŸ¥ä»£ç†ä¿®å¤æµ‹è¯•"""

    @pytest.fixture
    def ruff_agent(self):
        """åˆ›å»º RuffAgent å®ä¾‹"""
        return RuffAgent()

    @pytest.fixture
    def basedpyright_agent(self):
        """åˆ›å»º BasedPyrightAgent å®ä¾‹"""
        return BasedPyrightAgent()

    @pytest.fixture
    def pytest_agent(self):
        """åˆ›å»º PytestAgent å®ä¾‹"""
        return PytestAgent()


class TestRuffAgentErrorBranch(TestQualityAgentFixes):
    """RuffAgent é”™è¯¯åˆ†æ”¯æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_execute_error_branch_returns_failed_status(self, ruff_agent):
        """æµ‹è¯•æ‰§è¡Œé”™è¯¯åˆ†æ”¯è¿”å› 'failed' çŠ¶æ€è€Œé result['status']"""
        # æ¨¡æ‹Ÿ _run_subprocess è¿”å›é”™è¯¯çŠ¶æ€
        mock_result = {
            "status": "failed",
            "returncode": 1,
            "stdout": "",
            "stderr": "Command failed with error"
        }

        with patch.object(ruff_agent, "_run_subprocess", new_callable=AsyncMock) as mock_run:
            mock_run.return_value = mock_result

            result = await ruff_agent.execute("src")

            # éªŒè¯è¿”å› 'failed' çŠ¶æ€è€Œä¸æ˜¯ result['status']
            assert result["status"] == "failed"
            assert "errors" in result
            assert "warnings" in result
            assert "files_checked" in result
            assert "issues" in result
            assert "message" in result


class TestBasedPyrightAgentErrorBranch(TestQualityAgentFixes):
    """BasedPyrightAgent é”™è¯¯åˆ†æ”¯æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_execute_error_branch_returns_failed_status(self, basedpyright_agent):
        """æµ‹è¯•æ‰§è¡Œé”™è¯¯åˆ†æ”¯è¿”å› 'failed' çŠ¶æ€è€Œé result['status']"""
        # æ¨¡æ‹Ÿ _run_subprocess è¿”å›é”™è¯¯çŠ¶æ€
        mock_result = {
            "status": "failed",
            "returncode": 1,
            "stdout": "",
            "stderr": "Type check failed"
        }

        with patch.object(basedpyright_agent, "_run_subprocess", new_callable=AsyncMock) as mock_run:
            mock_run.return_value = mock_result

            result = await basedpyright_agent.execute("src")

            # éªŒè¯è¿”å› 'failed' çŠ¶æ€è€Œä¸æ˜¯ result['status']
            assert result["status"] == "failed"
            assert "errors" in result
            assert "warnings" in result
            assert "files_checked" in result
            assert "issues" in result
            assert "message" in result


class TestPytestAgentFailureLoading(TestQualityAgentFixes):
    """PytestAgent å¤±è´¥ä¿¡æ¯åŠ è½½æµ‹è¯•"""

    def test_load_failures_from_json_with_valid_data(self, pytest_agent, tmp_path):
        """æµ‹è¯•ä» JSON åŠ è½½æœ‰æ•ˆçš„å¤±è´¥ä¿¡æ¯"""
        # åˆ›å»ºä¸´æ—¶ JSON æ–‡ä»¶
        summary_json = tmp_path / "summary.json"
        test_file_path = "tests/test_integration.py"

        # åˆ›å»ºåŒ…å«å¤±è´¥ä¿¡æ¯çš„ JSON æ•°æ®
        summary_data = {
            "rounds": [
                {
                    "round_index": 1,
                    "round_type": "initial",
                    "timestamp": "2026-01-14T10:00:00Z",
                    "failed_files": [
                        {
                            "test_file": test_file_path,
                            "status": "failed",
                            "failures": [
                                {
                                    "nodeid": f"{test_file_path}::test_case_1",
                                    "failure_type": "failed",
                                    "message": "AssertionError: expected 1, got 2",
                                    "short_tb": "test_file.py:10: AssertionError"
                                },
                                {
                                    "nodeid": f"{test_file_path}::test_case_2",
                                    "failure_type": "error",
                                    "message": "TypeError: 'NoneType' object",
                                    "short_tb": "test_file.py:20: TypeError"
                                }
                            ]
                        }
                    ]
                }
            ]
        }

        with open(summary_json, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        # æµ‹è¯•åŠ è½½å¤±è´¥ä¿¡æ¯
        failures = pytest_agent._load_failures_from_json(
            str(summary_json),
            test_file_path
        )

        # éªŒè¯è¿”å›ç±»å‹å’Œå†…å®¹
        assert isinstance(failures, list)
        assert len(failures) == 2

        # éªŒè¯ç¬¬ä¸€ä¸ªå¤±è´¥ä¿¡æ¯
        failure1 = failures[0]
        assert failure1["nodeid"] == f"{test_file_path}::test_case_1"
        assert failure1["failure_type"] == "failed"
        assert "AssertionError" in failure1["message"]
        assert "test_file.py:10" in failure1["short_tb"]

        # éªŒè¯ç¬¬äºŒä¸ªå¤±è´¥ä¿¡æ¯
        failure2 = failures[1]
        assert failure2["nodeid"] == f"{test_file_path}::test_case_2"
        assert failure2["failure_type"] == "error"
        assert "TypeError" in failure2["message"]
        assert "test_file.py:20" in failure2["short_tb"]

    def test_load_failures_from_json_with_missing_file(self, pytest_agent, tmp_path):
        """æµ‹è¯•ä» JSON åŠ è½½ä¸å­˜åœ¨çš„æµ‹è¯•æ–‡ä»¶"""
        summary_json = tmp_path / "summary.json"

        summary_data = {
            "rounds": [
                {
                    "round_index": 1,
                    "round_type": "initial",
                    "failed_files": [
                        {
                            "test_file": "tests/other_test.py",
                            "status": "failed",
                            "failures": []
                        }
                    ]
                }
            ]
        }

        with open(summary_json, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        # æµ‹è¯•åŠ è½½ä¸å­˜åœ¨çš„æ–‡ä»¶
        failures = pytest_agent._load_failures_from_json(
            str(summary_json),
            "tests/test_integration.py"  # ä¸å­˜åœ¨çš„æ–‡ä»¶
        )

        # åº”è¯¥è¿”å›ç©ºåˆ—è¡¨
        assert failures == []

    def test_load_failures_from_json_with_invalid_type(self, pytest_agent, tmp_path, caplog):
        """æµ‹è¯•ä» JSON åŠ è½½æ— æ•ˆç±»å‹çš„å¤±è´¥ä¿¡æ¯"""
        summary_json = tmp_path / "summary.json"
        test_file_path = "tests/test_integration.py"

        # åˆ›å»ºåŒ…å«æ— æ•ˆç±»å‹æ•°æ®çš„ JSON
        summary_data = {
            "rounds": [
                {
                    "round_index": 1,
                    "round_type": "initial",
                    "failed_files": [
                        {
                            "test_file": test_file_path,
                            "status": "failed",
                            "failures": "invalid_type"  # åº”è¯¥æ˜¯ list ä½†è¿™é‡Œæ˜¯ str
                        }
                    ]
                }
            ]
        }

        with open(summary_json, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        # æµ‹è¯•åŠ è½½æ— æ•ˆç±»å‹
        with caplog.at_level(logging.WARNING):
            failures = pytest_agent._load_failures_from_json(
                str(summary_json),
                test_file_path
            )

        # åº”è¯¥è®°å½•è­¦å‘Šå¹¶è¿”å›ç©ºåˆ—è¡¨
        assert failures == []
        assert "Invalid failures format" in caplog.text

    def test_load_failures_from_json_with_incomplete_data(self, pytest_agent, tmp_path, caplog):
        """æµ‹è¯•ä» JSON åŠ è½½ä¸å®Œæ•´çš„å¤±è´¥ä¿¡æ¯"""
        summary_json = tmp_path / "summary.json"
        test_file_path = "tests/test_integration.py"

        # åˆ›å»ºåŒ…å«ä¸å®Œæ•´æ•°æ®çš„ JSON
        summary_data = {
            "rounds": [
                {
                    "round_index": 1,
                    "round_type": "initial",
                    "failed_files": [
                        {
                            "test_file": test_file_path,
                            "status": "failed",
                            "failures": [
                                {
                                    "nodeid": f"{test_file_path}::test_case_1",
                                    # ç¼ºå°‘ "failure_type" å­—æ®µ
                                    "message": "Error message",
                                    "short_tb": "Error traceback"
                                }
                            ]
                        }
                    ]
                }
            ]
        }

        with open(summary_json, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        # æµ‹è¯•åŠ è½½ä¸å®Œæ•´æ•°æ®
        with caplog.at_level(logging.WARNING):
            failures = pytest_agent._load_failures_from_json(
                str(summary_json),
                test_file_path
            )

        # ä¸å®Œæ•´çš„å¤±è´¥ä¿¡æ¯åº”è¯¥è¢«è·³è¿‡
        assert len(failures) == 0
        assert "Incomplete failure data" in caplog.text

    def test_load_failures_from_json_with_non_dict_items(self, pytest_agent, tmp_path, caplog):
        """æµ‹è¯•ä» JSON åŠ è½½éå­—å…¸é¡¹çš„å¤±è´¥ä¿¡æ¯"""
        summary_json = tmp_path / "summary.json"
        test_file_path = "tests/test_integration.py"

        # åˆ›å»ºåŒ…å«éå­—å…¸é¡¹çš„ JSON
        summary_data = {
            "rounds": [
                {
                    "round_index": 1,
                    "round_type": "initial",
                    "failed_files": [
                        {
                            "test_file": test_file_path,
                            "status": "failed",
                            "failures": [
                                "invalid_string",  # åº”è¯¥æ˜¯ dict ä½†è¿™é‡Œæ˜¯ str
                                123,  # åº”è¯¥æ˜¯ dict ä½†è¿™é‡Œæ˜¯ int
                                {
                                    "nodeid": f"{test_file_path}::test_case_1",
                                    "failure_type": "failed",
                                    "message": "Error message",
                                    "short_tb": "Error traceback"
                                }
                            ]
                        }
                    ]
                }
            ]
        }

        with open(summary_json, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        # æµ‹è¯•åŠ è½½éå­—å…¸é¡¹
        with caplog.at_level(logging.WARNING):
            failures = pytest_agent._load_failures_from_json(
                str(summary_json),
                test_file_path
            )

        # æœ‰æ•ˆçš„å¤±è´¥ä¿¡æ¯åº”è¯¥è¢«ä¿ç•™ï¼Œæ— æ•ˆçš„åº”è¯¥è¢«è·³è¿‡
        assert len(failures) == 1
        assert failures[0]["nodeid"] == f"{test_file_path}::test_case_1"


class TestSubprocessEncoding(TestQualityAgentFixes):
    """å­è¿›ç¨‹ç¼–ç æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_run_subprocess_with_utf8_encoding(self, pytest_agent):
        """æµ‹è¯• _run_subprocess ä½¿ç”¨ UTF-8 ç¼–ç """
        # æ¨¡æ‹Ÿ subprocess.run ä½¿ç”¨ UTF-8 ç¼–ç 
        mock_process = Mock()
        mock_process.returncode = 0
        mock_process.stdout = "æµ‹è¯•è¾“å‡º â€¢ âœ“ âœ—"
        mock_process.stderr = ""

        with patch('asyncio.get_event_loop') as mock_loop:
            mock_loop_instance = Mock()
            mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_process)
            mock_loop.return_value = mock_loop_instance

            result = await pytest_agent._run_subprocess("echo test")

            # éªŒè¯è°ƒç”¨äº†æ­£ç¡®çš„å‚æ•°
            call_args = mock_loop_instance.run_in_executor.call_args
            assert call_args is not None

            # éªŒè¯ encoding å‚æ•°è¢«ä¼ é€’
            executor_call = call_args[0][1]  # lambda å‡½æ•°
            # éªŒè¯ subprocess.run è¢«æ­£ç¡®è°ƒç”¨
            # æ³¨æ„ï¼šç”±äºä½¿ç”¨äº† lambdaï¼Œæˆ‘ä»¬éœ€è¦éªŒè¯æ•´ä½“è¡Œä¸º

            # éªŒè¯ç»“æœåŒ…å« UTF-8 å†…å®¹
            assert result["status"] == "completed"
            assert "æµ‹è¯•è¾“å‡º" in result["stdout"]
            assert "âœ“" in result["stdout"] or "âœ—" in result["stdout"]

    @pytest.mark.asyncio
    async def test_run_subprocess_with_unicode_characters(self, pytest_agent):
        """æµ‹è¯• _run_subprocess å¤„ç† Unicode å­—ç¬¦"""
        # åˆ›å»ºåŒ…å«å„ç§ Unicode å­—ç¬¦çš„æ¨¡æ‹Ÿè¾“å‡º
        unicode_output = "âœ“ æˆåŠŸ â€¢ é”™è¯¯ âœ— è­¦å‘Š ğŸ› Bug"

        mock_process = Mock()
        mock_process.returncode = 0
        mock_process.stdout = unicode_output
        mock_process.stderr = ""

        with patch('asyncio.get_event_loop') as mock_loop:
            mock_loop_instance = Mock()
            mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_process)
            mock_loop.return_value = mock_loop_instance

            result = await pytest_agent._run_subprocess("echo test")

            # éªŒè¯ Unicode å­—ç¬¦è¢«æ­£ç¡®å¤„ç†
            assert result["status"] == "completed"
            assert result["stdout"] == unicode_output


class TestPytestAgentJSONReportParsing(TestQualityAgentFixes):
    """PytestAgent JSONæŠ¥å‘Šè§£ææµ‹è¯•ï¼ˆæ–°å¢ï¼‰"""

    def test_parse_json_report_with_collectors(self, pytest_agent, tmp_path):
        """æµ‹è¯•è§£æåŒ…å«collection errorsçš„JSONæŠ¥å‘Š"""
        json_file = tmp_path / "test_report.json"
        test_file = "tests/test_cli.py"

        # åˆ›å»ºåŒ…å«collection errorsçš„JSONæ•°æ®
        json_data = {
            "collectors": [
                {
                    "nodeid": "tests/test_cli.py",
                    "outcome": "failed",
                    "longrepr": "ImportError: cannot import name 'xxx'\nModule not found"
                }
            ],
            "tests": []
        }

        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(json_data, f)

        # æµ‹è¯•è§£æ
        failures = pytest_agent._parse_json_report(str(json_file), test_file)

        # éªŒè¯æ•è·äº†collection error
        assert len(failures) == 1
        assert failures[0]["failure_type"] == "error"
        assert "Collection failed" in failures[0]["message"]
        assert "ImportError" in failures[0]["message"]
        assert failures[0]["nodeid"] == test_file

    def test_parse_json_report_with_test_failures(self, pytest_agent, tmp_path):
        """æµ‹è¯•è§£æåŒ…å«æµ‹è¯•å¤±è´¥çš„JSONæŠ¥å‘Š"""
        json_file = tmp_path / "test_report.json"
        test_file = "tests/test_cli.py"

        # åˆ›å»ºåŒ…å«æµ‹è¯•å¤±è´¥çš„JSONæ•°æ®
        json_data = {
            "collectors": [],
            "tests": [
                {
                    "nodeid": "tests/test_cli.py::test_example",
                    "outcome": "failed",
                    "call": {
                        "longrepr": "AssertionError: expected 5, got 3"
                    }
                }
            ]
        }

        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(json_data, f)

        # æµ‹è¯•è§£æ
        failures = pytest_agent._parse_json_report(str(json_file), test_file)

        # éªŒè¯æ•è·äº†æµ‹è¯•å¤±è´¥
        assert len(failures) == 1
        assert failures[0]["failure_type"] == "failed"
        assert "AssertionError" in failures[0]["message"]
        assert failures[0]["nodeid"] == "tests/test_cli.py::test_example"

    def test_parse_json_report_empty(self, pytest_agent, tmp_path, caplog):
        """æµ‹è¯•è§£æç©ºçš„JSONæŠ¥å‘Š"""
        json_file = tmp_path / "empty_report.json"
        test_file = "tests/test_cli.py"

        # åˆ›å»ºç©ºçš„JSONæ•°æ®
        json_data = {"collectors": [], "tests": []}

        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(json_data, f)

        # æµ‹è¯•è§£æ
        with caplog.at_level(logging.WARNING):
            failures = pytest_agent._parse_json_report(str(json_file), test_file)

        # éªŒè¯è¿”å›ç©ºåˆ—è¡¨å¹¶è®°å½•è­¦å‘Š
        assert failures == []
        assert "empty (no tests, no collectors)" in caplog.text

    @pytest.mark.asyncio
    async def test_run_pytest_single_file_with_stderr_fallback(self, pytest_agent, tmp_path):
        """æµ‹è¯•stderråå¤‡æœºåˆ¶ï¼ˆå½“JSONè§£æå¤±è´¥æ—¶ï¼‰"""
        test_file = "tests/test_cli.py"
        timeout = 30

        # åˆ›å»ºä¸´æ—¶JSONæ–‡ä»¶ï¼ˆä½†å†…å®¹ä¸ºç©ºï¼Œå¯¼è‡´è§£æå¤±è´¥ï¼‰
        json_file = tmp_path / "empty.json"
        json_file.write_text("{}")

        # æ¨¡æ‹Ÿå¤±è´¥æ‰§è¡Œç»“æœï¼ˆæœ‰stderrä½†æ— failuresï¼‰
        mock_result = {
            "returncode": 1,
            "status": "failed",
            "stderr": "ImportError: No module named 'xxx'",
            "stdout": ""
        }

        with patch.object(pytest_agent, "_parse_json_report", return_value=[]):
            with patch.object(pytest_agent, "_run_subprocess", new_callable=AsyncMock) as mock_run:
                mock_run.return_value = mock_result

                result = await pytest_agent._run_pytest_single_file(test_file, timeout)

                # éªŒè¯ä½¿ç”¨äº†stderrä½œä¸ºfallback
                assert len(result["failures"]) == 1
                assert result["failures"][0]["failure_type"] == "error"
                assert "No module named 'xxx'" in result["failures"][0]["message"]
                assert result["status"] == "error"


class TestEpicDriverStorySync(TestQualityAgentFixes):
    """EpicDriver æ•…äº‹åŒæ­¥æµ‹è¯•ï¼ˆæ–°å¢ï¼‰"""

    @pytest.mark.asyncio
    async def test_story_sync_uses_full_paths(self):
        """æµ‹è¯•æ•…äº‹åŒæ­¥ä½¿ç”¨å®Œæ•´è·¯å¾„è€ŒéçŸ­ID"""
        from autoBMAD.epic_automation.epic_driver import EpicDriver

        # æ¨¡æ‹Ÿstoriesæ•°æ®
        stories = [
            {"id": "1.1: Story 1.1", "path": "D:/GITHUB/pytQt_template/docs/stories/1.1.md"},
            {"id": "1.2: Story 1.2", "path": "D:/GITHUB/pytQt_template/docs/stories/1.2.md"},
        ]

        # åˆ›å»ºepic_driverå®ä¾‹ï¼ˆéƒ¨åˆ†mockï¼‰
        with patch("autoBMAD.epic_automation.epic_driver.EpicDriver.__init__", return_value=None):
            epic_driver = EpicDriver()
            epic_driver.logger = Mock()
            epic_driver.status_update_agent = Mock()
            epic_driver.state_manager = Mock()
            epic_driver.epic_id = "test_epic"

            # æ¨¡æ‹Ÿstories
            epic_driver.stories = stories

            # æ‰§è¡ŒåŒæ­¥
            epic_driver.logger.info = Mock()
            epic_driver.logger.debug = Mock()

            # åˆ›å»ºAsyncMock
            sync_mock = AsyncMock(return_value={
                "success_count": 2,
                "error_count": 0
            })
            epic_driver.status_update_agent.sync_from_database = sync_mock

            # è°ƒç”¨çŠ¶æ€åŒæ­¥ä»£ç 
            story_paths = [story["path"] for story in stories]
            epic_driver.logger.debug(f"Story paths for sync: {story_paths}")

            await epic_driver.status_update_agent.sync_from_database(
                state_manager=epic_driver.state_manager,
                epic_id=epic_driver.epic_id,
                story_ids=story_paths
            )

            # éªŒè¯ä¼ é€’çš„æ˜¯å®Œæ•´è·¯å¾„
            call_args = epic_driver.status_update_agent.sync_from_database.call_args
            assert call_args[1]["story_ids"] == story_paths
            assert all("docs/stories" in path for path in story_paths)


class TestQualityAgentIntegration:
    """è´¨é‡æ£€æŸ¥ä»£ç†é›†æˆæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_ruff_agent_complete_workflow(self, tmp_path):
        """æµ‹è¯• RuffAgent å®Œæ•´å·¥ä½œæµ"""
        ruff_agent = RuffAgent()

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = tmp_path / "test.py"
        test_file.write_text("import os\nprint('hello')\n")

        # æ¨¡æ‹ŸæˆåŠŸçš„æ£€æŸ¥ç»“æœ
        mock_result = {
            "status": "completed",
            "returncode": 0,
            "stdout": json.dumps([]),
            "stderr": ""
        }

        with patch.object(ruff_agent, "_run_subprocess", new_callable=AsyncMock) as mock_run:
            mock_run.return_value = mock_result

            result = await ruff_agent.execute(str(tmp_path))

            # éªŒè¯ç»“æœ
            assert result["status"] == "completed"
            assert "errors" in result
            assert "warnings" in result
            assert "files_checked" in result
            assert "issues" in result
            assert "message" in result

    @pytest.mark.asyncio
    async def test_basedpyright_agent_complete_workflow(self, tmp_path):
        """æµ‹è¯• BasedPyrightAgent å®Œæ•´å·¥ä½œæµ"""
        basedpyright_agent = BasedPyrightAgent()

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = tmp_path / "test.py"
        test_file.write_text("def func(x):\n    return x\n")

        # æ¨¡æ‹ŸæˆåŠŸçš„ç±»å‹æ£€æŸ¥ç»“æœ
        mock_result = {
            "status": "completed",
            "returncode": 0,
            "stdout": json.dumps({"generalDiagnostics": []}),
            "stderr": ""
        }

        with patch.object(basedpyright_agent, "_run_subprocess", new_callable=AsyncMock) as mock_run:
            mock_run.return_value = mock_result

            result = await basedpyright_agent.execute(str(tmp_path))

            # éªŒè¯ç»“æœ
            assert result["status"] == "completed"
            assert "errors" in result
            assert "warnings" in result
            assert "files_checked" in result
            assert "issues" in result
            assert "message" in result
