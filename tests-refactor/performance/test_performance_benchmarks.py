"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
éªŒè¯EpicDriveråœ¨ä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°
ç¬¦åˆPhase 4æ€§èƒ½æ ‡å‡†
"""

import pytest
import anyio
import tempfile
from pathlib import Path
from unittest.mock import MagicMock, AsyncMock, patch, Mock
import sys
import time
import psutil
import asyncio
import json
from datetime import datetime

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "autoBMAD"))

from autoBMAD.epic_automation.epic_driver import EpicDriver


# æ€§èƒ½åŸºçº¿é…ç½®
PERFORMANCE_BASELINE = {
    "single_story_processing": 30.0,  # ç§’
    "concurrent_5_stories": 45.0,     # ç§’
    "concurrent_10_stories": 90.0,    # ç§’
    "batch_10_stories": 300.0,        # ç§’
    "sdk_call_latency": 2.0,          # ç§’
    "memory_usage": 150.0,             # MB
    "cpu_usage": 70.0,                # %
    "memory_growth": 10.0,            # MB (é•¿æ—¶é—´è¿è¡Œå†…å­˜å¢é•¿)
}


@pytest.fixture
async def large_epic_structure():
    """åˆ›å»ºå¤§æ‰¹é‡Epicç»“æ„ç”¨äºæ€§èƒ½æµ‹è¯•"""
    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)

        # åˆ›å»ºé¡¹ç›®ç»“æ„
        src_dir = tmp_path / "src"
        src_dir.mkdir(parents=True, exist_ok=True)
        (src_dir / "main.py").write_text("# Main module\nprint('Performance test')\n", encoding='utf-8')

        tests_dir = tmp_path / "tests"
        tests_dir.mkdir(parents=True, exist_ok=True)
        (tests_dir / "test_main.py").write_text("# Test file\n", encoding='utf-8')

        docs_dir = tmp_path / "docs"
        docs_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºåŒ…å«10ä¸ªæ•…äº‹çš„Epicæ–‡ä»¶
        story_ids = [f"{i}" for i in range(1, 11)]

        epic_content = "# Epic: Performance Test\\n\\n"
        epic_content += "## Overview\\n"
        epic_content += "This is a performance test epic with multiple stories.\\n\\n"

        for story_id in story_ids:
            epic_content += f"### Story {story_id}: Performance Test Story {story_id}\\n\\n"

        epic_content += "## Acceptance Criteria\\n"
        epic_content += "- [ ] All stories processed efficiently\\n"
        epic_content += "- [ ] Performance within acceptable limits\\n"

        epic_file = docs_dir / "epic-performance-test.md"
        epic_file.write_text(epic_content, encoding='utf-8')

        # åˆ›å»ºStoriesç›®å½•
        stories_dir = tmp_path / "stories"
        stories_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»º10ä¸ªæ•…äº‹
        stories = []
        for story_id in story_ids:
            story_content = f"""# Story {story_id}: Performance Test Story {story_id}

**Status**: Draft

## Description
This is story {story_id} for performance testing.

## Acceptance Criteria
1. Story {story_id} processes efficiently
2. No performance degradation

## Tasks
- [ ] Task {story_id}.1: Setup
- [ ] Task {story_id}.2: Execute
- [ ] Task {story_id}.3: Verify
"""
            story_file = stories_dir / f"{story_id}-performance-test.md"
            story_file.write_text(story_content, encoding='utf-8')
            stories.append({"file": story_file, "id": story_id})

        yield {
            "root_dir": tmp_path,
            "epic_file": epic_file,
            "stories": stories
        }


@pytest.fixture
async def performance_monitor():
    """æ€§èƒ½ç›‘æ§å™¨"""
    class PerformanceMonitor:
        def __init__(self):
            self.start_time = None
            self.end_time = None
            self.start_memory = None
            self.end_memory = None
            self.peak_memory = 0
            self.process = psutil.Process()

        def start(self):
            """å¼€å§‹ç›‘æ§"""
            self.start_time = time.time()
            self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            self.peak_memory = self.start_memory

        def record_memory(self):
            """è®°å½•å½“å‰å†…å­˜ä½¿ç”¨"""
            current_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            if current_memory > self.peak_memory:
                self.peak_memory = current_memory

        def stop(self):
            """åœæ­¢ç›‘æ§"""
            self.end_time = time.time()
            self.end_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        def get_results(self):
            """è·å–æ€§èƒ½ç»“æœ"""
            elapsed_time = (self.end_time - self.start_time) if self.end_time else 0
            memory_increase = (self.end_memory - self.start_memory) if self.end_memory else 0

            return {
                "elapsed_time": elapsed_time,
                "start_memory": self.start_memory,
                "end_memory": self.end_memory,
                "peak_memory": self.peak_memory,
                "memory_increase": memory_increase,
                "memory_growth": memory_increase
            }

    return PerformanceMonitor()


@pytest.fixture
async def fast_mock_sdk():
    """å¿«é€ŸMock SDKç”¨äºæ€§èƒ½æµ‹è¯•"""
    mock_sdk = MagicMock()

    # å¿«é€Ÿå“åº”ï¼ˆ< 0.1ç§’ï¼‰
    async def fast_mock_call(prompt, task_group, **kwargs):
        # æ¨¡æ‹Ÿå¿«é€ŸSDKè°ƒç”¨
        await asyncio.sleep(0.05)  # 50ms
        result = Mock()
        result.success = True
        result.content = f"Fast response for: {prompt[:50]}"
        return result

    mock_sdk.call = AsyncMock(side_effect=fast_mock_call)
    mock_sdk.close = MagicMock()

    return mock_sdk


@pytest.mark.performance
@pytest.mark.anyio
async def test_batch_story_performance(large_epic_structure, fast_mock_sdk, performance_monitor):
    """
    å¤§æ‰¹é‡æ•…äº‹å¤„ç†æ€§èƒ½æµ‹è¯•
    10ä¸ªæ•…äº‹é¡ºåºå¤„ç†ï¼Œæ€»æ—¶é—´åº” < 300ç§’
    """
    print("\\n=== å¤§æ‰¹é‡æ•…äº‹å¤„ç†æ€§èƒ½æµ‹è¯• ===")
    print(f"æµ‹è¯•{len(large_epic_structure['stories'])}ä¸ªæ•…äº‹é¡ºåºå¤„ç†")

    performance_monitor.start()

    with patch('autoBMAD.epic_automation.epic_driver.SafeClaudeSDK', return_value=fast_mock_sdk):
        driver = EpicDriver(
            epic_path=str(large_epic_structure["epic_file"]),
            max_iterations=3  # é™åˆ¶å¾ªç¯æ¬¡æ•°ä»¥åŠ é€Ÿæµ‹è¯•
        )

        start_time = time.time()

        # é¡ºåºå¤„ç†æ‰€æœ‰æ•…äº‹
        for story in large_epic_structure["stories"]:
            performance_monitor.record_memory()
            print(f"å¤„ç†æ•…äº‹{story['id']}...")
            await driver.process_story(story["id"])

        performance_monitor.stop()

        results = performance_monitor.get_results()

        print(f"\\nâ±ï¸  æ€»å¤„ç†æ—¶é—´: {results['elapsed_time']:.2f}ç§’")
        print(f"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {results['peak_memory']:.2f} MB")
        print(f"ğŸ“ˆ å†…å­˜å¢é•¿: {results['memory_growth']:.2f} MB")

        # éªŒè¯æ€§èƒ½åŸºçº¿
        assert results['elapsed_time'] < PERFORMANCE_BASELINE["batch_10_stories"], (
            f"æ‰¹é‡å¤„ç†æ—¶é—´è¶…æ ‡: {results['elapsed_time']:.2f}s > {PERFORMANCE_BASELINE['batch_10_stories']}s"
        )

        assert results['memory_growth'] < PERFORMANCE_BASELINE["memory_growth"], (
            f"å†…å­˜å¢é•¿è¶…æ ‡: {results['memory_growth']:.2f}MB > {PERFORMANCE_BASELINE['memory_growth']}MB"
        )

        print("âœ… å¤§æ‰¹é‡æ•…äº‹å¤„ç†æ€§èƒ½æµ‹è¯•é€šè¿‡")


@pytest.mark.performance
@pytest.mark.anyio
async def test_concurrent_performance(large_epic_structure, fast_mock_sdk, performance_monitor):
    """
    å¹¶å‘æ€§èƒ½æµ‹è¯•
    10ä¸ªæ•…äº‹å¹¶å‘å¤„ç†ï¼Œæ€»æ—¶é—´åº” < 90ç§’
    """
    print("\\n=== å¹¶å‘æ€§èƒ½æµ‹è¯• ===")
    print(f"æµ‹è¯•{len(large_epic_structure['stories'])}ä¸ªæ•…äº‹å¹¶å‘å¤„ç†")

    performance_monitor.start()

    with patch('autoBMAD.epic_automation.epic_driver.SafeClaudeSDK', return_value=fast_mock_sdk):
        driver = EpicDriver(
            epic_path=str(large_epic_structure["epic_file"]),
            max_iterations=3
        )

        # å¹¶å‘å¤„ç†æ‰€æœ‰æ•…äº‹
        async with anyio.create_task_group() as tg:
            for story in large_epic_structure["stories"]:
                tg.start_soon(driver.process_story, story["id"])

        performance_monitor.stop()

        results = performance_monitor.get_results()

        print(f"\\nâ±ï¸  å¹¶å‘å¤„ç†æ—¶é—´: {results['elapsed_time']:.2f}ç§’")
        print(f"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {results['peak_memory']:.2f} MB")
        print(f"ğŸ“ˆ å†…å­˜å¢é•¿: {results['memory_growth']:.2f} MB")

        # éªŒè¯æ€§èƒ½åŸºçº¿
        assert results['elapsed_time'] < PERFORMANCE_BASELINE["concurrent_10_stories"], (
            f"å¹¶å‘å¤„ç†æ—¶é—´è¶…æ ‡: {results['elapsed_time']:.2f}s > {PERFORMANCE_BASELINE['concurrent_10_stories']}s"
        )

        assert results['memory_growth'] < PERFORMANCE_BASELINE["memory_growth"], (
            f"å†…å­˜å¢é•¿è¶…æ ‡: {results['memory_growth']:.2f}MB > {PERFORMANCE_BASELINE['memory_growth']}MB"
        )

        print("âœ… å¹¶å‘æ€§èƒ½æµ‹è¯•é€šè¿‡")


@pytest.mark.performance
@pytest.mark.anyio
async def test_memory_leak_detection(large_epic_structure, fast_mock_sdk, performance_monitor):
    """
    å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•
    é•¿æ—¶é—´è¿è¡Œå†…å­˜å¢é•¿åº” < 10MB
    """
    print("\\n=== å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯• ===")

    performance_monitor.start()

    with patch('autoBMAD.epic_automation.epic_driver.SafeClaudeSDK', return_value=fast_mock_sdk):
        driver = EpicDriver(
            epic_path=str(large_epic_structure["epic_file"]),
            max_iterations=3
        )

        # é‡å¤å¤„ç†æ•…äº‹å¤šæ¬¡ä»¥æ£€æµ‹å†…å­˜æ³„æ¼
        iterations = 5
        print(f"é‡å¤å¤„ç†{iterations}è½®...")

        for i in range(iterations):
            print(f"\\nç¬¬{i + 1}è½®...")
            for story in large_epic_structure["stories"]:
                await driver.process_story(story["id"])
                performance_monitor.record_memory()

        performance_monitor.stop()

        results = performance_monitor.get_results()

        print(f"\\nğŸ’¾ åˆå§‹å†…å­˜: {results['start_memory']:.2f} MB")
        print(f"ğŸ’¾ æœ€ç»ˆå†…å­˜: {results['end_memory']:.2f} MB")
        print(f"ğŸ’¾ å³°å€¼å†…å­˜: {results['peak_memory']:.2f} MB")
        print(f"ğŸ“ˆ å†…å­˜å¢é•¿: {results['memory_growth']:.2f} MB")

        # éªŒè¯å†…å­˜å¢é•¿åœ¨å¯æ¥å—èŒƒå›´å†…
        assert results['memory_growth'] < PERFORMANCE_BASELINE["memory_growth"], (
            f"å†…å­˜å¢é•¿è¶…æ ‡: {results['memory_growth']:.2f}MB > {PERFORMANCE_BASELINE['memory_growth']}MB"
        )

        print("âœ… å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•é€šè¿‡")


@pytest.mark.performance
@pytest.mark.anyio
async def test_cpu_usage_monitoring(fast_mock_sdk, performance_monitor):
    """
    CPUä½¿ç”¨ç›‘æ§æµ‹è¯•
    å³°å€¼CPUä½¿ç”¨ç‡åº” < 77%
    """
    print("\\n=== CPUä½¿ç”¨ç›‘æ§æµ‹è¯• ===")

    performance_monitor.start()

    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)

        with patch('autoBMAD.epic_automation.epic_driver.SafeClaudeSDK', return_value=fast_mock_sdk):
            # åˆ›å»ºEpicæ–‡ä»¶
            epic_file = tmp_path / "epic.md"
            epic_file.write_text("# Test Epic\n", encoding='utf-8')

            driver = EpicDriver(
                epic_path=str(epic_file),
                max_iterations=3
            )

            # åˆ›å»ºæµ‹è¯•æ•…äº‹
            stories_dir = tmp_path / "stories"
            stories_dir.mkdir(parents=True, exist_ok=True)

            # åˆ›å»º5ä¸ªæµ‹è¯•æ•…äº‹
            for i in range(5):
                story_file = stories_dir / f"test-story-{i}.md"
                story_file.write_text(f"""
# Test Story {i}

**Status**: Draft

## Description
CPU monitoring test story {i}.

## Tasks
- [ ] Task {i}.1: Execute
""", encoding='utf-8')

                # å¤„ç†æ•…äº‹å¹¶ç›‘æ§CPU
                await driver.process_story(f"test-story-{i}")
                performance_monitor.record_memory()

        performance_monitor.stop()

        results = performance_monitor.get_results()

        print(f"\\nâ±ï¸  æ‰§è¡Œæ—¶é—´: {results['elapsed_time']:.2f}ç§’")
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {results['end_memory']:.2f} MB")

        # CPUä½¿ç”¨ç‡é€šè¿‡è¿›ç¨‹ç›‘æ§è·å–
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"ğŸ–¥ï¸  CPUä½¿ç”¨ç‡: {cpu_percent}%")

        # éªŒè¯å†…å­˜ä½¿ç”¨
        assert results['memory_growth'] < PERFORMANCE_BASELINE["memory_usage"], (
            f"å†…å­˜ä½¿ç”¨è¶…æ ‡: {results['memory_growth']:.2f}MB > {PERFORMANCE_BASELINE['memory_usage']}MB"
        )

        print("âœ… CPUä½¿ç”¨ç›‘æ§æµ‹è¯•é€šè¿‡")


@pytest.mark.performance
@pytest.mark.anyio
async def test_sdk_call_latency(fast_mock_sdk, performance_monitor):
    """
    SDKè°ƒç”¨å»¶è¿Ÿæµ‹è¯•
    å¹³å‡SDKè°ƒç”¨å»¶è¿Ÿåº” < 2.2ç§’
    """
    print("\\n=== SDKè°ƒç”¨å»¶è¿Ÿæµ‹è¯• ===")

    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)

        with patch('autoBMAD.epic_automation.epic_driver.SafeClaudeSDK', return_value=fast_mock_sdk):
            # åˆ›å»ºEpicæ–‡ä»¶
            epic_file = tmp_path / "epic.md"
            epic_file.write_text("# Test Epic\n", encoding='utf-8')

            driver = EpicDriver(
                epic_path=str(epic_file),
                max_iterations=1
            )

            # åˆ›å»ºæµ‹è¯•æ•…äº‹
            stories_dir = tmp_path / "stories"
            stories_dir.mkdir(parents=True, exist_ok=True)

            story_file = stories_dir / "test-story.md"
            story_file.write_text("""
# Test Story

**Status**: Draft

## Description
SDK latency test story.

## Tasks
- [ ] Task 1: Measure latency
""", encoding='utf-8')

            # æµ‹é‡SDKè°ƒç”¨å»¶è¿Ÿ
            latencies = []

            for i in range(5):
                start_time = time.time()
                await driver.process_story("test-story")
                end_time = time.time()
                latency = end_time - start_time
                latencies.append(latency)
                print(f"è°ƒç”¨{i + 1}å»¶è¿Ÿ: {latency:.2f}ç§’")

            # è®¡ç®—å¹³å‡å»¶è¿Ÿ
            avg_latency = sum(latencies) / len(latencies)
            max_latency = max(latencies)
            min_latency = min(latencies)

            print(f"\\nğŸ“Š å»¶è¿Ÿç»Ÿè®¡:")
            print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ç§’")
            print(f"   æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f}ç§’")
            print(f"   æœ€å°å»¶è¿Ÿ: {min_latency:.2f}ç§’")

            # éªŒè¯å»¶è¿ŸåŸºçº¿
            assert avg_latency < PERFORMANCE_BASELINE["sdk_call_latency"], (
                f"å¹³å‡SDKå»¶è¿Ÿè¶…æ ‡: {avg_latency:.2f}s > {PERFORMANCE_BASELINE['sdk_call_latency']}s"
            )

            print("âœ… SDKè°ƒç”¨å»¶è¿Ÿæµ‹è¯•é€šè¿‡")


@pytest.mark.performance
@pytest.mark.anyio
async def test_performance_regression_detection(fast_mock_sdk, performance_monitor):
    """
    æ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯•
    å¯¹æ¯”ä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°
    """
    print("\\n=== æ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯• ===")

    test_results = {}

    # æµ‹è¯•ä¸åŒè´Ÿè½½
    test_cases = [
        {"name": "1-story", "count": 1},
        {"name": "3-stories", "count": 3},
        {"name": "5-stories", "count": 5},
    ]

    for test_case in test_cases:
        print(f"\\næµ‹è¯•{test_case['name']}è´Ÿè½½...")

        with tempfile.TemporaryDirectory() as tmp_dir:
            tmp_path = Path(tmp_dir)

            with patch('autoBMAD.epic_automation.epic_driver.SafeClaudeSDK', return_value=fast_mock_sdk):
                driver = EpicDriver(
                    epic_path=str(epic_file),
                    max_iterations=2
                )

                # åˆ›å»ºæµ‹è¯•æ•…äº‹
                stories_dir = tmp_path / "stories"
                stories_dir.mkdir(parents=True, exist_ok=True)

                for i in range(test_case['count']):
                    story_file = stories_dir / f"test-story-{i}.md"
                    story_file.write_text(f"""
# Test Story {i}

**Status**: Draft

## Description
Performance regression test story {i}.

## Tasks
- [ ] Task {i}.1: Execute
""", encoding='utf-8')

                # å¼€å§‹ç›‘æ§
                monitor = performance_monitor.__class__()  # åˆ›å»ºæ–°çš„ç›‘æ§å™¨
                monitor.start()

                # å¤„ç†æ•…äº‹
                for i in range(test_case['count']):
                    await driver.process_story(f"test-story-{i}")

                monitor.stop()
                results = monitor.get_results()

                test_results[test_case['name']] = {
                    "story_count": test_case['count'],
                    "elapsed_time": results['elapsed_time'],
                    "memory_growth": results['memory_growth']
                }

                print(f"âœ… {test_case['name']}: {results['elapsed_time']:.2f}s")

    # è¾“å‡ºæ€§èƒ½å¯¹æ¯”
    print("\\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    for name, result in test_results.items():
        per_story_time = result['elapsed_time'] / result['story_count']
        print(f"{name:15s}: {result['elapsed_time']:6.2f}s ({per_story_time:.2f}s/story)")

    # éªŒè¯æ€§èƒ½çº¿æ€§å¢é•¿
    time_1 = test_results["1-story"]["elapsed_time"]
    time_3 = test_results["3-stories"]["elapsed_time"]
    time_5 = test_results["5-stories"]["elapsed_time"]

    # 3ä¸ªæ•…äº‹çš„æ—¶é—´åº”è¯¥æ¥è¿‘3å€ï¼Œ5ä¸ªæ•…äº‹åº”è¯¥æ¥è¿‘5å€ï¼ˆå…è®¸10%è¯¯å·®ï¼‰
    assert time_3 / time_1 < 3.3, "æ€§èƒ½éšè´Ÿè½½å¢é•¿è¿‡å¿«"
    assert time_5 / time_1 < 5.5, "æ€§èƒ½éšè´Ÿè½½å¢é•¿è¿‡å¿«"

    print("âœ… æ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯•é€šè¿‡")


@pytest.mark.performance
@pytest.mark.anyio
async def test_concurrent_vs_sequential_performance(large_epic_structure, fast_mock_sdk):
    """
    å¹¶å‘vsé¡ºåºæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    éªŒè¯å¹¶å‘å¤„ç†çš„æ€§èƒ½ä¼˜åŠ¿
    """
    print("\\n=== å¹¶å‘vsé¡ºåºæ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

    # é¡ºåºå¤„ç†
    print("\\nğŸ“ é¡ºåºå¤„ç†...")
    with patch('autoBMAD.epic_automation.epic_driver.SafeClaudeSDK', return_value=fast_mock_sdk):
        driver = EpicDriver(
            project_root=large_epic_structure["root_dir"],
            max_iterations=2
        )

        start_time = time.time()
        for story in large_epic_structure["stories"]:
            await driver.process_story(story["id"])
        sequential_time = time.time() - start_time
        print(f"âœ… é¡ºåºå¤„ç†æ—¶é—´: {sequential_time:.2f}ç§’")

    # å¹¶å‘å¤„ç†
    print("\\nâš¡ å¹¶å‘å¤„ç†...")
    with patch('autoBMAD.epic_automation.epic_driver.SafeClaudeSDK', return_value=fast_mock_sdk):
        driver = EpicDriver(
            project_root=large_epic_structure["root_dir"],
            max_iterations=2
        )

        start_time = time.time()
        async with anyio.create_task_group() as tg:
            for story in large_epic_structure["stories"]:
                tg.start_soon(driver.process_story, story["id"])
        concurrent_time = time.time() - start_time
        print(f"âœ… å¹¶å‘å¤„ç†æ—¶é—´: {concurrent_time:.2f}ç§’")

    # è®¡ç®—æ€§èƒ½æå‡
    improvement = sequential_time / concurrent_time
    print(f"\\nğŸ“ˆ æ€§èƒ½æå‡: {improvement:.2f}x")

    # éªŒè¯å¹¶å‘æœ‰æ˜¾è‘—æ€§èƒ½ä¼˜åŠ¿
    assert improvement > 1.5, "å¹¶å‘å¤„ç†æ€§èƒ½æå‡ä¸è¶³"
    print("âœ… å¹¶å‘vsé¡ºåºæ€§èƒ½å¯¹æ¯”æµ‹è¯•é€šè¿‡")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    print("\\n" + "="*80)
    print("æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*80)
    print("\\næ€§èƒ½åŸºçº¿:")
    for key, value in PERFORMANCE_BASELINE.items():
        print(f"  {key}: {value}")

    pytest.main([__file__, "-v", "-s", "-m", "performance"])
