"""
Quality Agents - é‡æ„åçš„è´¨é‡æ£€æŸ¥ Agents
å¢å¼ºåæ”¯æŒTaskGroupç®¡ç†
"""
from __future__ import annotations

import asyncio
import json
import logging
import subprocess
from abc import ABC
from pathlib import Path
from typing import Any, TypedDict, NotRequired, Literal, cast

from anyio.abc import TaskGroup

from autoBMAD.epic_automation.agents.base_agent import BaseAgent
from autoBMAD.epic_automation.core.sdk_result import SDKResult

logger = logging.getLogger(__name__)


# ç±»å‹å®šä¹‰
class SubprocessResult(TypedDict):
    status: Literal["completed", "failed"]
    returncode: int
    stdout: str
    stderr: str
    success: bool
    error: NotRequired[str]
    command: NotRequired[str]


class RuffIssue(TypedDict):
    filename: str
    code: str
    message: str
    severity: Literal["error", "warning"]
    location: dict[str, int]


class RuffResult(TypedDict):
    status: Literal["completed", "failed"]
    errors: int
    warnings: int
    files_checked: int
    issues: list[RuffIssue]
    message: str
    error: NotRequired[str]


class BasedPyrightIssue(TypedDict):
    file: str
    rule: str | None
    message: str
    severity: Literal["error", "warning"]
    range: dict[str, Any]


class BasedPyrightResult(TypedDict):
    status: Literal["completed", "failed"]
    errors: int
    warnings: int
    files_checked: int
    issues: list[BasedPyrightIssue]
    message: str
    error: NotRequired[str]


class PytestTestCase(TypedDict):
    nodeid: str
    failure_type: Literal["failed", "error"]
    message: str
    short_tb: str


class PytestFileResult(TypedDict):
    test_file: str
    status: Literal["passed", "failed", "error", "timeout"]
    failures: list[PytestTestCase]


class PytestResult(TypedDict):
    status: Literal["completed", "failed"]
    files: list[PytestFileResult]
    error: NotRequired[str]


class BaseQualityAgent(BaseAgent, ABC):
    """è´¨é‡æ£€æŸ¥ Agent åŸºç±»"""

    def __init__(
        self,
        name: str,
        task_group: TaskGroup | None = None,
    ):
        """
        åˆå§‹åŒ–è´¨é‡æ£€æŸ¥ Agent

        Args:
            name: Agentåç§°
            task_group: TaskGroupå®ä¾‹
        """
        super().__init__(name, task_group)
        self._log_execution(f"{name} initialized")

    async def _run_subprocess(self, command: str, timeout: int = 300) -> SubprocessResult:
        """
        è¿è¡Œå­è¿›ç¨‹å‘½ä»¤

        Args:
            command: è¦æ‰§è¡Œçš„å‘½ä»¤
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            SubprocessResult: æ‰§è¡Œç»“æœ
        """
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œå­è¿›ç¨‹ï¼Œé¿å… cancel scope ä¼ æ’­
            loop = asyncio.get_event_loop()
            process = await asyncio.wait_for(
                loop.run_in_executor(
                    None,
                    lambda: subprocess.run(
                        command,
                        shell=True,
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        errors='ignore',
                        timeout=timeout
                    )
                ),
                timeout=timeout + 10  # é¢å¤–å¢åŠ 10ç§’çš„è¶…æ—¶ä¿æŠ¤
            )

            return SubprocessResult(
                status="completed",
                returncode=process.returncode,
                stdout=process.stdout,
                stderr=process.stderr,
                success=process.returncode == 0
            )
        except TimeoutError:
            self.logger.error(f"Command timed out after {timeout} seconds: {command}")
            return SubprocessResult(
                status="failed",
                returncode=-1,
                stdout="",
                stderr=f"Timeout after {timeout} seconds",
                success=False,
                error=f"Timeout after {timeout} seconds",
                command=command
            )
        except Exception as e:
            self.logger.error(f"Command failed: {e}")
            return SubprocessResult(
                status="failed",
                returncode=-1,
                stdout="",
                stderr=str(e),
                success=False,
                error=str(e),
                command=command
            )


class RuffAgent(BaseQualityAgent):
    """Ruff ä»£ç é£æ ¼æ£€æŸ¥ Agentï¼ˆæ”¹é€ ç‰ˆ - æ”¯æŒSDKè‡ªåŠ¨ä¿®å¤ï¼‰"""

    def __init__(self, task_group: TaskGroup | None = None):
        super().__init__("Ruff", task_group)

    async def execute(
        self,
        source_dir: str,
        project_root: str | None = None,
        **kwargs: object
    ) -> RuffResult:
        """
        æ‰§è¡Œ Ruff æ£€æŸ¥ï¼ˆå¢åŠ  --fix è‡ªåŠ¨ä¿®å¤ï¼‰

        Args:
            source_dir: æºä»£ç ç›®å½•
            project_root: é¡¹ç›®æ ¹ç›®å½•

        Returns:
            RuffResult: æ£€æŸ¥ç»“æœ
        """
        self.logger.info("Running Ruff checks with auto-fix")

        try:
            # æ„å»º Ruff å‘½ä»¤ï¼ˆå¢åŠ  --fixï¼‰
            command = f"ruff check --fix --output-format=json {source_dir}"

            result = await self._run_subprocess(command)

            if result["status"] == "completed":
                # è§£æ JSON è¾“å‡º
                try:
                    issues_list: list[dict[str, object]] = json.loads(result["stdout"]) if result["stdout"] else []
                    error_count = len([i for i in issues_list if i.get("severity") == "error"])
                    warning_count = len([i for i in issues_list if i.get("severity") == "warning"])
                    filenames = {i.get("filename", "") for i in issues_list}
                    files_count = len(filenames)

                    return RuffResult(
                        status="completed",
                        errors=error_count,
                        warnings=warning_count,
                        files_checked=files_count,
                        issues=issues_list,
                        message=f"Found {len(issues_list)} issues (after auto-fix)"
                    )
                except json.JSONDecodeError:
                    return RuffResult(
                        status="completed",
                        errors=0,
                        warnings=0,
                        files_checked=0,
                        issues=[],
                        message="Check completed (no JSON output)"
                    )
            else:
                return RuffResult(
                    status="failed",
                    errors=0,
                    warnings=0,
                    files_checked=0,
                    issues=[],
                    message=result.get("stderr", "Ruff check failed")
                )

        except Exception as e:
            self.logger.error(f"Ruff check failed: {e}")
            return RuffResult(
                status="failed",
                errors=0,
                warnings=0,
                files_checked=0,
                issues=[],
                message=f"Ruff check failed: {str(e)}",
                error=f"Ruff check failed: {str(e)}"
            )

    def parse_errors_by_file(
        self,
        issues: list[dict[str, object]]
    ) -> dict[str, list[dict[str, object]]]:
        """
        æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„é”™è¯¯

        Args:
            issues: ruff JSON è¾“å‡ºçš„ issues åˆ—è¡¨

        Returns:
            {"src/a.py": [error1, error2], ...}
        """
        errors_by_file: dict[str, list[dict[str, object]]] = {}

        for issue in issues:
            issue_dict = cast(dict[str, Any], issue)
            file_path = issue_dict.get("filename", "")
            if not file_path:
                continue

            if file_path not in errors_by_file:
                errors_by_file[file_path] = []

            # æå–å…³é”®é”™è¯¯ä¿¡æ¯
            location = cast(dict[str, Any], issue_dict.get("location", {}))
            errors_by_file[file_path].append({
                "line": location.get("row"),
                "column": location.get("column"),
                "code": issue_dict.get("code"),
                "message": issue_dict.get("message"),
                "severity": issue_dict.get("severity", "error"),
            })

        return errors_by_file

    def build_fix_prompt(
        self,
        tool: str,
        file_path: str,
        file_content: str,
        errors: list[dict[str, object]],
    ) -> str:
        """
        æ„é€  Ruff ä¿®å¤ Prompt

        Args:
            tool: å·¥å…·åç§° ('ruff')
            file_path: æ–‡ä»¶è·¯å¾„
            file_content: æ–‡ä»¶å†…å®¹
            errors: é”™è¯¯åˆ—è¡¨

        Returns:
            å®Œæ•´çš„ä¿®å¤ Prompt
        """
        errors_summary = self._format_errors_summary(errors)

        return RUFF_FIX_PROMPT.format(
            file_path=file_path,
            file_content=file_content,
            errors_summary=errors_summary,
        )

    def _format_errors_summary(self, errors: list[dict[str, object]]) -> str:
        """æ ¼å¼åŒ–é”™è¯¯æ‘˜è¦"""
        lines = []
        for i, error in enumerate(errors, 1):
            error_dict = cast(dict[str, Any], error)
            lines.append(f"""
### Error {i}
- **Line**: {error_dict.get('line')}
- **Column**: {error_dict.get('column')}
- **Code**: `{error_dict.get('code')}`
- **Message**: {error_dict.get('message')}
- **Severity**: {error_dict.get('severity')}""".strip())

        return "\n\n".join(lines)

    async def format(self, source_dir: str) -> dict[str, Any]:
        """
        æ‰§è¡Œ ruff formatï¼ˆæ–°å¢ï¼‰

        Args:
            source_dir: æºä»£ç ç›®å½•

        Returns:
            {
                "status": "completed" | "failed",
                "formatted": bool,
                "message": str
            }
        """
        self.logger.info("Running ruff format")

        try:
            command = f"ruff format {source_dir}"
            result = await self._run_subprocess(command)

            formatted = result["returncode"] == 0

            return {
                "status": "completed" if formatted else "failed",
                "formatted": formatted,
                "message": "Code formatted successfully" if formatted else "Format failed",
            }

        except Exception as e:
            self.logger.error(f"Ruff format failed: {e}")
            return {
                "status": "failed",
                "formatted": False,
                "error": str(e)
            }


class BasedPyrightAgent(BaseQualityAgent):
    """BasedPyright ç±»å‹æ£€æŸ¥ Agentï¼ˆæ”¹é€ ç‰ˆ - æ”¯æŒSDKè‡ªåŠ¨ä¿®å¤ï¼‰"""

    def __init__(self, task_group: TaskGroup | None = None):
        super().__init__("BasedPyright", task_group)

    async def execute(self, source_dir: str, **kwargs: object) -> BasedPyrightResult:
        """
        æ‰§è¡Œ BasedPyright æ£€æŸ¥

        Args:
            source_dir: æºä»£ç ç›®å½•

        Returns:
            BasedPyrightResult: æ£€æŸ¥ç»“æœ
        """
        self.logger.info("Running BasedPyright checks")

        try:
            # æ„å»º BasedPyright å‘½ä»¤
            command = f"basedpyright --outputjson {source_dir}"

            result = await self._run_subprocess(command)

            if result["status"] == "completed":
                # è§£æ JSON è¾“å‡º
                try:
                    output_dict: dict[str, object] = json.loads(result["stdout"]) if result["stdout"] else {}
                    issues_list: list[dict[str, object]] = cast(list[dict[str, object]], output_dict.get("generalDiagnostics", []))
                    error_count = len([i for i in issues_list if i.get("severity") == "error"])
                    warning_count = len([i for i in issues_list if i.get("severity") == "warning"])
                    files_set = {i.get("file", "") for i in issues_list}
                    files_count = len(files_set)

                    return BasedPyrightResult(
                        status="completed",
                        errors=error_count,
                        warnings=warning_count,
                        files_checked=files_count,
                        issues=issues_list,
                        message=f"Found {len(issues_list)} type issues"
                    )
                except json.JSONDecodeError:
                    return BasedPyrightResult(
                        status="completed",
                        errors=0,
                        warnings=0,
                        files_checked=0,
                        issues=[],
                        message="Check completed (no JSON output)"
                    )
            else:
                return BasedPyrightResult(
                    status="failed",
                    errors=0,
                    warnings=0,
                    files_checked=0,
                    issues=[],
                    message=result.get("stderr", "BasedPyright check failed")
                )

        except Exception as e:
            self.logger.error(f"BasedPyright check failed: {e}")
            return BasedPyrightResult(
                status="failed",
                errors=0,
                warnings=0,
                files_checked=0,
                issues=[],
                message=f"BasedPyright check failed: {str(e)}",
                error=f"BasedPyright check failed: {str(e)}"
            )

    def parse_errors_by_file(
        self,
        issues: list[dict[str, object]]
    ) -> dict[str, list[dict[str, object]]]:
        """
        æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„é”™è¯¯

        Args:
            issues: basedpyright JSON è¾“å‡ºçš„ generalDiagnostics

        Returns:
            {"src/x.py": [error1], ...}
        """
        errors_by_file: dict[str, list[dict[str, object]]] = {}

        for issue in issues:
            issue_dict = cast(dict[str, Any], issue)
            file_path = issue_dict.get("file", "")
            if not file_path:
                continue

            if file_path not in errors_by_file:
                errors_by_file[file_path] = []

            # æå–å…³é”®é”™è¯¯ä¿¡æ¯
            range_info: dict[str, object] = cast(dict[str, object], issue_dict.get("range", {}))
            start_info: dict[str, Any] = cast(dict[str, Any], range_info.get("start", {}))

            errors_by_file[file_path].append({
                "line": start_info.get("line"),
                "column": start_info.get("character"),
                "rule": issue_dict.get("rule"),
                "message": issue_dict.get("message"),
                "severity": issue_dict.get("severity", "error"),
            })

        return errors_by_file

    def build_fix_prompt(
        self,
        tool: str,
        file_path: str,
        file_content: str,
        errors: list[dict[str, object]],
    ) -> str:
        """æ„é€  BasedPyright ä¿®å¤ Prompt"""
        errors_summary = self._format_errors_summary(errors)

        return BASEDPYRIGHT_FIX_PROMPT.format(
            file_path=file_path,
            file_content=file_content,
            errors_summary=errors_summary,
        )

    def _format_errors_summary(self, errors: list[dict[str, object]]) -> str:
        """æ ¼å¼åŒ–é”™è¯¯æ‘˜è¦"""
        lines = []
        for i, error in enumerate(errors, 1):
            error_dict = cast(dict[str, Any], error)
            lines.append(f"""
### Type Error {i}
- **Line**: {error_dict.get('line')}
- **Column**: {error_dict.get('column')}
- **Rule**: `{error_dict.get('rule')}`
- **Message**: {error_dict.get('message')}
- **Severity**: {error_dict.get('severity')}""".strip())

        return "\n\n".join(lines)


class PytestAgent(BaseQualityAgent):
    """Pytest æµ‹è¯•æ‰§è¡Œ Agent - æ”¯æŒç›®å½•éå†æ‰¹æ¬¡æ‰§è¡Œå’ŒSDKä¿®å¤"""

    def __init__(self, task_group: TaskGroup | None = None):
        super().__init__("Pytest", task_group)

    async def execute(
        self,
        source_dir: str,
        test_dir: str
    ) -> PytestResult:
        """
        æ‰§è¡Œ Pytest æµ‹è¯•ï¼ˆç›®å½•éå†æ‰¹æ¬¡æ‰§è¡Œï¼‰

        Args:
            source_dir: æºä»£ç ç›®å½•
            test_dir: æµ‹è¯•ç›®å½•

        Returns:
            PytestResult: æµ‹è¯•ç»“æœ
        """
        self.logger.info("Running Pytest with directory-based batching")

        try:
            from pathlib import Path

            from .pytest_batch_executor import PytestBatchExecutor

            # åˆ›å»ºæ‰¹æ¬¡æ‰§è¡Œå™¨
            executor = PytestBatchExecutor(
                test_dir=Path(test_dir),
                source_dir=Path(source_dir)
            )

            # æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
            result = await executor.execute_batches()

            # ç¡®ä¿resultç¬¦åˆPytestResultæ ¼å¼
            return PytestResult(
                status=result.get("status", "failed"),
                files=result.get("files", [])
            )

        except Exception as e:
            self.logger.error(f"Pytest execution failed: {e}")
            return PytestResult(
                status="failed",
                files=[],
                error=str(e)
            )

    async def run_tests_sequential(
        self,
        test_files: list[str],
        timeout_per_file: int,
        round_index: int,
        round_type: str,
    ) -> dict[str, object]:
        """
        æŒ‰æ–‡ä»¶é¡ºåºæ‰§è¡Œ pytest -v --tb=short

        Args:
            test_files: æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
            timeout_per_file: æ¯ä¸ªæ–‡ä»¶çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            round_index: è½®æ¬¡ç´¢å¼•
            round_type: "initial" | "retry"

        Returns:
            {
                "files": [
                    {
                        "test_file": "...",
                        "status": "passed" | "failed" | "error" | "timeout",
                        "failures": [...],  # ä»…å½“ status != passed
                    }
                ]
            }
        """
        self.logger.info(f"Running sequential tests: {len(test_files)} files (round {round_index}, type: {round_type})")

        results = []

        for test_file in test_files:
            # æ‰§è¡Œå•ä¸ªæ–‡ä»¶çš„ pytest
            file_result = await self._run_pytest_single_file(
                test_file=test_file,
                timeout=timeout_per_file,
            )
            results.append(file_result)

        return {"files": results}

    async def _run_pytest_single_file(
        self,
        test_file: str,
        timeout: int,
    ) -> PytestFileResult:
        """
        æ‰§è¡Œå•ä¸ªæµ‹è¯•æ–‡ä»¶çš„ pytest

        å¢å¼ºç‰ˆï¼šæ·»åŠ stderråå¤‡æœºåˆ¶

        å‘½ä»¤ï¼špytest <test_file> -v --tb=short --json-report --json-report-file=<tmp>

        Args:
            test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            {
                "test_file": str,
                "status": str,
                "failures": list[dict],  # ä» json-report æå–
            }
        """
        self.logger.debug(f"Running pytest on {test_file}")

        # 1. æ„é€ å‘½ä»¤
        import tempfile
        from pathlib import Path

        tmp_json = tempfile.NamedTemporaryFile(suffix=".json", delete=False, mode="w")
        tmp_json_path = tmp_json.name
        tmp_json.close()

        try:
            cmd = f'pytest {test_file} -v --tb=short --json-report --json-report-file={tmp_json_path}'

            # 2. æ‰§è¡Œï¼ˆå¤ç”¨ BaseQualityAgent._run_subprocessï¼‰
            result = await self._run_subprocess(cmd, timeout=timeout)

            # 3. è§£æ json-report
            failures = self._parse_json_report(tmp_json_path, test_file)

            # âœ… æ–°å¢ï¼šåå¤‡æœºåˆ¶ - å½“failuresä¸ºç©ºä½†æ‰§è¡Œå¤±è´¥æ—¶ï¼Œä½¿ç”¨stderr
            if not failures and result["returncode"] != 0:
                stderr = result.get("stderr", "")
                stdout = result.get("stdout", "")

                # ä¼˜å…ˆä½¿ç”¨stderrï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨stdout
                error_output = stderr if stderr else stdout

                if error_output:
                    # æå–å…³é”®é”™è¯¯ä¿¡æ¯ï¼ˆå‰500å­—ç¬¦ï¼‰
                    error_summary = error_output[:500]
                    failures = [{
                        "nodeid": test_file,
                        "failure_type": "error",
                        "message": f"Pytest execution failed (no JSON report):\n{error_summary}",
                        "short_tb": "Check pytest output - no structured report available"
                    }]
                    self.logger.warning(
                        f"No failures parsed from JSON but returncode={result['returncode']}, "
                        f"using stderr/stdout as fallback"
                    )

            # 4. åˆ¤æ–­çŠ¶æ€
            if result.get("status") == "failed" and "Timeout" in result.get("error", ""):
                status = "timeout"
            elif result["returncode"] == 0:
                status = "passed"
            elif failures:
                status = "failed" if any(f["failure_type"] == "failed" for f in failures) else "error"
            else:
                # ğŸ†• æœ€ç»ˆåå¤‡ï¼šå³ä½¿æ— è¾“å‡ºä¹Ÿæ„é€ é”™è¯¯æ¡ç›®
                status = "error"
                if not failures:
                    failures = [{
                        "nodeid": test_file,
                        "failure_type": "error",
                        "message": f"Test execution failed with returncode {result['returncode']}",
                        "short_tb": "Run pytest manually for details"
                    }]
                    self.logger.info(f"Constructed minimal failure entry for {test_file}")

            return {
                "test_file": test_file,
                "status": status,
                "failures": failures,
            }

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                Path(tmp_json_path).unlink(missing_ok=True)
            except Exception:
                pass

    def _parse_json_report(
        self,
        json_path: str,
        test_file: str,
    ) -> list[PytestTestCase]:
        """
        ä» pytest-json-report ä¸­æå–å¤±è´¥ä¿¡æ¯

        å¢å¼ºç‰ˆï¼šæ”¯æŒæ•è·collection errorå’Œæµ‹è¯•ç”¨ä¾‹å¤±è´¥

        Args:
            json_path: JSON æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„

        Returns:
            [
                {
                    "nodeid": "...",
                    "failure_type": "failed" | "error",
                    "message": "...",
                    "short_tb": "...",
                }
            ]
        """
        if not Path(json_path).exists():
            self.logger.warning(f"JSON report not found: {json_path}")
            return []

        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (json.JSONDecodeError, Exception) as e:
            self.logger.error(f"Failed to parse JSON report: {e}")
            return []

        failures = []

        # âœ… æ–°å¢ï¼šæ£€æŸ¥collectioné”™è¯¯
        collectors = data.get("collectors", [])
        if collectors:
            for collector in collectors:
                if collector.get("outcome") == "failed":
                    longrepr = collector.get("longrepr", "")
                    # æå–å…³é”®é”™è¯¯ä¿¡æ¯
                    error_lines = str(longrepr).split("\n")
                    error_summary = "\n".join(error_lines[:10])  # å‰10è¡Œ

                    failures.append({
                        "nodeid": collector.get("nodeid", test_file),
                        "failure_type": "error",  # collectioné”™è¯¯æ ‡è®°ä¸ºerror
                        "message": f"Collection failed: {error_summary}",
                        "short_tb": f"Test file collection error at {test_file}"
                    })
                    self.logger.warning(f"Captured collection error for {test_file}")

        # âœ… åŸæœ‰é€»è¾‘ï¼šæå–æµ‹è¯•ç”¨ä¾‹å¤±è´¥
        tests = data.get("tests", [])
        for test in tests:
            outcome = test.get("outcome")
            if outcome in ["failed", "error"]:
                # ä»…ä¿ç•™å½“å‰æµ‹è¯•æ–‡ä»¶çš„ç”¨ä¾‹
                if not test["nodeid"].startswith(test_file):
                    continue

                call = test.get("call", {})
                failures.append({
                    "nodeid": test["nodeid"],
                    "failure_type": outcome,  # ä¿ç•™åŸå§‹ç±»å‹(failed/error)
                    "message": call.get("longrepr", "Unknown error"),
                    "short_tb": self._extract_short_traceback(test),
                })

        # âœ… æ–°å¢ï¼šè¯Šæ–­æ—¥å¿—
        if not failures and not tests and not collectors:
            self.logger.warning(
                f"JSON report for {test_file} is empty (no tests, no collectors). "
                f"This may indicate a pytest execution failure."
            )

        return failures

    def _extract_short_traceback(self, test: dict[str, object]) -> str:
        """ä» test å¯¹è±¡ä¸­æå–ç²¾ç®€çš„å †æ ˆä¿¡æ¯"""
        # æå–å…³é”®è¡Œå·å’Œé”™è¯¯ä½ç½®
        try:
            call: dict[str, object] = cast(dict[str, object], test.get("call", {}))
            longrepr = call.get("longrepr", "")

            # å°è¯•æå–æœ€åå‡ è¡Œä½œä¸ºçŸ­å †æ ˆ
            if isinstance(longrepr, str):
                lines = longrepr.split("\n")
                # å–æœ€å3è¡Œ
                return "\n".join(lines[-3:]) if len(lines) > 3 else longrepr

            return str(longrepr)
        except Exception:
            return "Traceback information unavailable"

    async def run_sdk_fix_for_file(
        self,
        test_file: str,
        source_dir: str,
        summary_json_path: str,
        round_index: int,
    ) -> dict[str, bool | str]:
        """
        å¯¹å•ä¸ªæµ‹è¯•æ–‡ä»¶å‘èµ· SDK ä¿®å¤è°ƒç”¨

        æµç¨‹ï¼š
        1. ä»æ±‡æ€» JSON ä¸­è¯»å–è¯¥æ–‡ä»¶çš„å¤±è´¥ä¿¡æ¯
        2. è¯»å–æµ‹è¯•æ–‡ä»¶å†…å®¹
        3. æ„é€  Promptï¼ˆä½¿ç”¨ Prompt æ¨¡æ¿ï¼‰
        4. é€šè¿‡ SafeClaudeSDK å‘èµ·è°ƒç”¨
        5. æ”¶åˆ° ResultMessage â†’ è§¦å‘å–æ¶ˆ â†’ ç­‰å¾…ç¡®è®¤
        6. è¿”å›ç®€å•çš„æˆåŠŸ/å¤±è´¥æ ‡å¿—

        Args:
            test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            source_dir: æºä»£ç ç›®å½•
            summary_json_path: æ±‡æ€» JSON è·¯å¾„
            round_index: å½“å‰è½®æ¬¡

        Returns:
            {
                "success": bool,
                "error": str | None,
            }
        """
        self.logger.info(f"Starting SDK fix for {test_file} (round {round_index})")

        try:
            # 1. è¯»å–å¤±è´¥ä¿¡æ¯
            failures: list[PytestTestCase] = self._load_failures_from_json(summary_json_path, test_file)

            if not failures:
                self.logger.warning(f"No failure information found for {test_file}")
                return {"success": False, "error": "No failure information available"}

            # 2. è¯»å–æµ‹è¯•æ–‡ä»¶å†…å®¹
            with open(test_file, "r", encoding="utf-8") as f:
                test_content = f.read()

            # 3. æ„é€  Prompt
            prompt = self._build_fix_prompt(
                test_file=test_file,
                source_dir=source_dir,
                test_content=test_content,
                failures=failures,
            )

            # 4. è°ƒç”¨ SDKï¼ˆè¿”å› SDKResultï¼‰
            from ..core.sdk_result import SDKResult
            sdk_result: SDKResult = await self._execute_sdk_call_with_cancel(prompt)

            # 5. ä½¿ç”¨ SDKResult è¯­ä¹‰
            if sdk_result.is_success():
                self.logger.info(
                    f"SDK fix succeeded for {test_file} "
                    f"(duration: {sdk_result.duration_seconds:.2f}s)"
                )
                return {
                    "success": True,
                    "error": None
                }
            else:
                error_summary = sdk_result.get_error_summary()
                self.logger.error(
                    f"SDK fix failed for {test_file}: {error_summary}"
                )
                return {
                    "success": False,
                    "error": error_summary
                }

        except Exception as e:
            self.logger.error(f"SDK fix failed for {test_file}: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
            }

    async def _execute_sdk_call_with_cancel(self, prompt: str) -> SDKResult:
        """
        æ‰§è¡Œ SDK è°ƒç”¨å¹¶å¤„ç†å–æ¶ˆæµç¨‹ï¼ˆé‡æ„ä¸ºç»Ÿä¸€è·¯å¾„ï¼‰

        ä¿®å¤ç‚¹:
        1. ä½¿ç”¨ execute_sdk_call ç»Ÿä¸€å…¥å£
        2. è‡ªåŠ¨å¤„ç† ClaudeAgentOptions æ„é€ 
        3. è¿”å›ç±»å‹æ˜ç¡®ä¸º SDKResult
        """
        from .sdk_helper import execute_sdk_call

        # ç»Ÿä¸€è°ƒç”¨ï¼Œè‡ªåŠ¨å¤„ç† options ç±»å‹è½¬æ¢
        result = await execute_sdk_call(
            prompt=prompt,
            agent_name="PytestAgent",
            timeout=300.0,
            permission_mode="bypassPermissions"
        )

        return result

    def _load_failures_from_json(
        self,
        summary_json_path: str,
        test_file: str,
    ) -> list[PytestTestCase]:
        """ä»æ±‡æ€» JSON ä¸­åŠ è½½æŒ‡å®šæµ‹è¯•æ–‡ä»¶çš„å¤±è´¥ä¿¡æ¯"""
        import json

        if not Path(summary_json_path).exists():
            self.logger.warning(f"Summary JSON not found: {summary_json_path}")
            return []

        try:
            with open(summary_json_path, "r", encoding="utf-8") as f:
                data: dict[str, object] = json.load(f)

            # ä»æœ€åä¸€è½®ä¸­æŸ¥æ‰¾è¯¥æ–‡ä»¶çš„å¤±è´¥ä¿¡æ¯
            rounds: list[object] = cast(list[object], data.get("rounds", []))
            if rounds:
                last_round: dict[str, object] = cast(dict[str, object], rounds[-1])
                failed_files: list[object] = cast(list[object], last_round.get("failed_files", []))
                for item in failed_files:
                    item_dict: dict[str, object] = cast(dict[str, object], item)
                    if item_dict["test_file"] == test_file:
                        failures_raw = item_dict.get("failures", [])
                        # éªŒè¯å¹¶è½¬æ¢ç±»å‹
                        if not isinstance(failures_raw, list):
                            self.logger.warning(
                                f"Invalid failures format for {test_file}: expected list, got {type(failures_raw)}"
                            )
                            return []

                        # è½¬æ¢ä¸º PytestTestCase ç±»å‹
                        failures: list[PytestTestCase] = []
                        for failure in failures_raw:
                            if not isinstance(failure, dict):
                                continue

                            # éªŒè¯å¿…éœ€å­—æ®µ
                            if not all(k in failure for k in ["nodeid", "failure_type", "message", "short_tb"]):
                                self.logger.warning(f"Incomplete failure data: {failure}")
                                continue

                            failures.append({
                                "nodeid": str(failure["nodeid"]),
                                "failure_type": str(failure["failure_type"]),
                                "message": str(failure["message"]),
                                "short_tb": str(failure["short_tb"])
                            })

                        return failures

            return []

        except (json.JSONDecodeError, Exception) as e:
            self.logger.error(f"Failed to load failures from JSON: {e}")
            return []

    def _build_fix_prompt(
        self,
        test_file: str,
        source_dir: str,
        test_content: str,
        failures: list[PytestTestCase],
    ) -> str:
        """
        æ„é€  SDK ä¿®å¤æç¤ºè¯

        ä½¿ç”¨ Prompt æ¨¡æ¿
        """
        # æ„é€ å¤±è´¥æ‘˜è¦éƒ¨åˆ†
        failures_lines: list[str] = []
        for i, failure in enumerate(failures, 1):
            nodeid = failure['nodeid']
            failure_type = failure['failure_type']
            message = failure['message']
            short_tb = failure['short_tb']
            failures_lines.append(f"""
### Case {i}
- **nodeid**: `{nodeid}`
- **type**: `{failure_type}`
- **message**: `{message}`
- **short traceback**: `{short_tb}`""".strip())

        failures_summary = "\n\n".join(failures_lines)

        # å¡«å……æ¨¡æ¿
        prompt = PROMPT_TEMPLATE.format(
            test_file=test_file,
            source_dir=source_dir,
            test_content=test_content,
            failures_summary=failures_summary,
        )

        return prompt


# Prompt æ¨¡æ¿
PROMPT_TEMPLATE = """
<system>
ä½ æ˜¯ä¸€åèµ„æ·± Python æµ‹è¯•ä¸ä»£ç ä¿®å¤ä¸“å®¶ã€‚

ç›®æ ‡ï¼š
- æ£€æŸ¥æµ‹è¯•æ˜¯å¦å‡ºç°å¡é¡¿ï¼Œå¦‚æœ‰åˆ™ä¿®å¤æµ‹è¯•ã€‚
- æ ¹æ®ç»™å®šçš„æµ‹è¯•æ–‡ä»¶å’Œå¤±è´¥ä¿¡æ¯ï¼Œè¾“å‡ºä¸€ä¸ªä¿®å¤æ–¹æ¡ˆï¼Œä½¿æµ‹è¯•é€šè¿‡ã€‚
- ä¿æŒä¸šåŠ¡é€»è¾‘æ­£ç¡®ï¼Œé¿å…æ— å…³é‡æ„ã€‚

çº¦æŸï¼š
- åªä¿®æ”¹å¿…è¦çš„ä»£ç ï¼ˆæµ‹è¯•æ–‡ä»¶åŠç›¸å…³æºç ï¼‰ã€‚
- ä¿æŒæµ‹è¯•åç§°ã€è¯­ä¹‰å’ŒéªŒæ”¶æ„å›¾ä¸å˜ã€‚
- è¾“å‡ºæ ¼å¼ï¼šå…ˆç»™å‡ºä¿®æ”¹æ‘˜è¦ï¼Œå†ç»™å‡ºæ¯ä¸ªæ–‡ä»¶çš„å®Œæ•´æ–°ç‰ˆæœ¬ã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
## Summary of Changes
- ä¿®å¤ç‚¹ 1
- ä¿®å¤ç‚¹ 2

## Patched Files
### File: tests/unit/test_x.py
```python
# å®Œæ•´ä¿®å¤åçš„æµ‹è¯•æ–‡ä»¶å†…å®¹
```

### File: src/module.py (å¦‚éœ€ä¿®æ”¹æºç )
```python
# å®Œæ•´ä¿®å¤åçš„æºç æ–‡ä»¶å†…å®¹
```

<END_OF_PATCH>
</system>

<user>
## Test File Information
- **Test file path**: {test_file}
- **Project source dir**: {source_dir}

## Test File Content (Current)
```python
{test_content}
```

## Failures Summary
{failures_summary}

## Expected Result
ä¿®å¤å¯¼è‡´ä¸Šè¿°å¤±è´¥çš„æ ¹å› ï¼Œä½¿æ‰€æœ‰ç”¨ä¾‹é€šè¿‡ã€‚è‹¥éœ€è¦ä¿®æ”¹ä¸šåŠ¡æºç ï¼Œè¯·è¯´æ˜ä¿®æ”¹ä½ç½®å’ŒåŸå› ã€‚
</user>
"""


# Ruff ä¿®å¤ Prompt æ¨¡æ¿
RUFF_FIX_PROMPT = """
<system>
ä½ æ˜¯ä¸€åèµ„æ·± Python ä»£ç è´¨é‡ä¸“å®¶ï¼Œä¸“ç²¾äº Ruff ä»£ç é£æ ¼ä¿®å¤ã€‚

ç›®æ ‡ï¼š
- æ ¹æ®ç»™å®šçš„æ–‡ä»¶å’Œ Ruff é”™è¯¯ä¿¡æ¯ï¼Œè¾“å‡ºä¿®å¤æ–¹æ¡ˆï¼Œä½¿ä»£ç é€šè¿‡ Ruff æ£€æŸ¥ã€‚
- ä¿æŒä¸šåŠ¡é€»è¾‘ä¸å˜ï¼Œåªä¿®å¤ä»£ç é£æ ¼é—®é¢˜ã€‚

çº¦æŸï¼š
- åªä¿®æ”¹å¿…è¦çš„ä»£ç ä»¥è§£å†³ Ruff æŠ¥å‘Šçš„é—®é¢˜ã€‚
- ä¸è¿›è¡Œæ— å…³çš„é‡æ„æˆ–ä¼˜åŒ–ã€‚
- ä¿æŒä»£ç çš„å¯è¯»æ€§å’Œä¸€è‡´æ€§ã€‚
- éµå¾ª PEP 8 è§„èŒƒã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
## Summary of Changes
- ä¿®å¤ç‚¹ 1ï¼šç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥
- ä¿®å¤ç‚¹ 2ï¼šä¿®æ­£è¡Œé•¿åº¦é—®é¢˜

## Fixed File
### File: {file_path}
```python
# å®Œæ•´ä¿®å¤åçš„æ–‡ä»¶å†…å®¹
```

<RUFF_FIX_COMPLETE>
</system>

<user>
## File Information
- **File path**: {file_path}

## File Content (Current)
```python
{file_content}
```

## Ruff Errors
{errors_summary}

## Expected Result
ä¿®å¤ä¸Šè¿°æ‰€æœ‰ Ruff é”™è¯¯ï¼Œä½¿ä»£ç é€šè¿‡ Ruff æ£€æŸ¥ã€‚è¾“å‡ºå®Œæ•´ä¿®å¤åçš„æ–‡ä»¶å†…å®¹ã€‚
</user>
"""

# BasedPyright ä¿®å¤ Prompt æ¨¡æ¿
BASEDPYRIGHT_FIX_PROMPT = """
<system>
ä½ æ˜¯ä¸€åèµ„æ·± Python ç±»å‹æ³¨è§£ä¸“å®¶ï¼Œä¸“ç²¾äº BasedPyright ç±»å‹æ£€æŸ¥ä¿®å¤ã€‚

ç›®æ ‡ï¼š
- æ ¹æ®ç»™å®šçš„æ–‡ä»¶å’Œç±»å‹é”™è¯¯ä¿¡æ¯ï¼Œè¾“å‡ºä¿®å¤æ–¹æ¡ˆï¼Œä½¿ä»£ç é€šè¿‡ç±»å‹æ£€æŸ¥ã€‚
- æ·»åŠ å¿…è¦çš„ç±»å‹æ³¨è§£ï¼Œä¿®å¤ç±»å‹ä¸åŒ¹é…é—®é¢˜ã€‚

çº¦æŸï¼š
- åªä¿®æ”¹å¿…è¦çš„ä»£ç ä»¥è§£å†³ç±»å‹æ£€æŸ¥é—®é¢˜ã€‚
- ä½¿ç”¨æ ‡å‡†çš„ typing æ¨¡å—ç±»å‹æ³¨è§£ã€‚
- ä¿æŒä¸šåŠ¡é€»è¾‘ä¸å˜ã€‚
- ç¡®ä¿ç±»å‹æ³¨è§£å‡†ç¡®ã€å®Œæ•´ã€‚

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
## Summary of Changes
- ä¿®å¤ç‚¹ 1ï¼šæ·»åŠ å‡½æ•°è¿”å›ç±»å‹æ³¨è§£
- ä¿®å¤ç‚¹ 2ï¼šä¿®æ­£å‚æ•°ç±»å‹ä¸åŒ¹é…

## Fixed File
### File: {file_path}
```python
# å®Œæ•´ä¿®å¤åçš„æ–‡ä»¶å†…å®¹
```

<BASEDPYRIGHT_FIX_COMPLETE>
</system>

<user>
## File Information
- **File path**: {file_path}

## File Content (Current)
```python
{file_content}
```

## BasedPyright Type Errors
{errors_summary}

## Expected Result
ä¿®å¤ä¸Šè¿°æ‰€æœ‰ç±»å‹æ£€æŸ¥é”™è¯¯ï¼Œæ·»åŠ å¿…è¦çš„ç±»å‹æ³¨è§£ã€‚è¾“å‡ºå®Œæ•´ä¿®å¤åçš„æ–‡ä»¶å†…å®¹ã€‚
</user>
"""
